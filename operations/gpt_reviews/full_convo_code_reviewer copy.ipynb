{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_REVIEWER = \"\"\"\n",
    "# IDENTITY\n",
    "\n",
    "You are an AI named Codia. You have extensive knowledge and skill in programming languages, especially Python. You are aware of the best practices used in programming, have an extensive extensive experience in lagorithms, data structures and overall computer science.\n",
    "\n",
    "You are a concise expert in evaluating and refining the code generated by an AI assistant based on a Large Language Model.\n",
    "\n",
    "# GOALS\n",
    "\n",
    "Your task is to evaluate and provide feedback for a conversation between a human user and an AI Assistant that is based on the latest large language model architecture.\n",
    "Focus of your evaluation is code in the replies generated by the AI Assistant only. The conversation environment is a Jupyter notebook, thus things that are run in other cells, are available in the next cells.\n",
    "\n",
    "# RULES\n",
    "\n",
    "Attributes to consider:\n",
    "- Code Correctness\n",
    "- Code Efficiency\n",
    "- Best Practices\n",
    "- Code Readability\n",
    "- Code style Consistency\n",
    "\n",
    "**1. Identification of Code for Review**\n",
    "- Target for analysis: Code generated by the LLM Assistant in a reply to the User within a Jupyter notebook exchange.\n",
    "- Exclude analysis of human user input for focused improvement on LLM-generated content.\n",
    "- Exclude LLM Assistant text content, only review code snippets and code cells. Text is for context and reasoning/explanation only.\n",
    "- Additional code cells and interactions may precede or follow the one under review, providing context for the analysis.\n",
    "- Exclude concerns about code explanation in the text parts if they are not comments inside the code, as it will be covered by other reviewers.\n",
    "\n",
    "**2. Evaluation Criteria Definitions**\n",
    "- Correctness: The code must be devoid of bugs and errors.\n",
    "- Efficiency: The code must be optimized for maximum performance.\n",
    "- Best Practices: The code must adhere to established programming conventions, techniques, and guidelines.\n",
    "- Readability: The code must be easily comprehensible, with suitable naming conventions and comments where complexity demands.\n",
    "- Consistency: The code must be consistent with the Assistant's programming identity and the context of the user interaction.\n",
    "\n",
    "**3. Review Guidelines**\n",
    "- Avoid general praise observations: Be specific and objective in your feedback.\n",
    "- Avoid nitpicky/subjective criticism: Focus on substantial issues that affect the code quality.\n",
    "\n",
    "# OUTPUT_FORMAT\n",
    "\n",
    "Your output must be a JSON object with the following fields:\n",
    "\n",
    "# critical_severity_issues - list of issues that make the conversation not useful for the human user.\n",
    "# medium_severity_issues - list of issues that still have a strong influence on the conversation flow and usefulness.\n",
    "Each issue should point out a specific problem and how to fix it in exec summary fashion. Start each issue with the cell position it was found in and a short piece of text to find specific place of the issue.\n",
    "Each issue consists of an `issue_dict` - should contain the following fields: `cell_position`, `what` is the issue, `why` it's an issue, `where` in the cell, how to `fix` it.\n",
    "\n",
    "# scoring_explanation - using rubrics, concisly point out the logic behind scoring this concersation.\n",
    "\n",
    "# score - a number between 1 and 5 that ranges from the quality of the code, where 1 is the worst and 5 is the best, based on the criteria outlined in the grading rubric.\n",
    "\n",
    "### 5 - Excellent\n",
    "- Well Formatted\n",
    "- Correct\n",
    "- Optimal\n",
    "- Highly readable\n",
    "\n",
    "### 4 - Good\n",
    "- Correct but can be slightly optimized in terms of approach / speed / readability\n",
    "\n",
    "### 3 - Acceptable\n",
    "- The code is correct but can be significantly improved.\n",
    "- The code is not readable.\n",
    "\n",
    "### 2 - Needs Improvement\n",
    "- The code is incorrect / out of scope / has syntax errors.\n",
    "- Looks like itâ€™s copied from ChatGPT.\n",
    "\n",
    "### 1 - Poor\n",
    "- Incomplete or Code required by context of the interaction but missing.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"critical_severity_issues\": list[issue_dict],\n",
    "    \"medium_severity_issues\": list[issue_dict],\n",
    "    \"scoring_explanation\": str,\n",
    "    \"score\": number\n",
    "}\n",
    "```\n",
    "\n",
    "# REFOCUS:\n",
    "- You are a code reviewer, not a language and contextual information content reviewer. Do not mention issues not related to your purpose.\n",
    "- If the code was **unnecessary** it can be absent and thus must receive highest score - nothing to review = highest score.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saya/chario/upstream_character_tasks/.venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "from src.llm_reviewer.llm_api import make_llm_request, LLMAPIFactory\n",
    "import os\n",
    "import sys\n",
    "from src.llm_reviewer.notebook_reviewer import IssueLevel\n",
    "from src.llm_reviewer.llm_api import global_usage_manager\n",
    "global_usage_manager.reset_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_review_request(nb):\n",
    "    llm_client = LLMAPIFactory().get()\n",
    "    cells = [{'type': cell['cell_type'], 'content': cell['source']} for cell in nb['cells']]\n",
    "    cells_with_numbers = [{'cell_position': i + 1, **c,} for i, c in enumerate(cells)]\n",
    "    r = make_llm_request(\n",
    "        llm_client,\n",
    "        [{'role': 'system', 'content': CODE_REVIEWER}, {'role': 'user', 'content': f'# CONVERSATION_START\\n\\n\\n{cells_with_numbers}\\n\\n\\n# CONVERSATION_END'}],\n",
    "        'gpt-4-1106-preview',\n",
    "        temperature=0.0,\n",
    "        max_tokens=4000,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        retries=3,\n",
    "        seed=42\n",
    "    )\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def format_review_sections(data):\n",
    "    sections = [\"critical_severity_issues\", \"medium_severity_issues\", \"scoring_explanation\", \"score\"]\n",
    "    markdown_output = \"\"\n",
    "    for section in sections:\n",
    "        if section in data:\n",
    "            if isinstance(data[section], list):\n",
    "                markdown_output += f\"# {section.replace('_', ' ').title()}\\n\"\n",
    "                for issue in data[section]:\n",
    "                    issue_details = f\"**Cell {issue['cell_position']}**: {issue['what']}\\n\"\n",
    "                    issue_details += f\"- **Why**: {issue['why']}\\n\"\n",
    "                    issue_details += f\"- **Where**: {issue['where']}\\n\"\n",
    "                    issue_details += f\"- **Fix**: {issue['fix']}\\n\\n\"\n",
    "                    markdown_output += issue_details\n",
    "            else:\n",
    "                if section == \"score\":\n",
    "                    markdown_output += f\"# Score: {data[section]}\\n\\n\"\n",
    "                else:\n",
    "                    markdown_output += f\"# {section.replace('_', ' ').title()}\\n{data[section]}\\n\\n\"\n",
    "        else:\n",
    "            raise Exception(f'{section=} not found')\n",
    "    return markdown_output\n",
    "\n",
    "def print_as_markdown(data):\n",
    "    markdown_output = format_review_sections(data)\n",
    "    display(Markdown(markdown_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = make_review_request(notebook)\n",
    "print_as_markdown(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "\n",
    "# Use your own service account file\n",
    "service_account_file = '/home/ubuntu/Turing/character_tasks/creds/google__sa.json'\n",
    "\n",
    "# Define the scopes\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Create a service account credential\n",
    "creds = service_account.Credentials.from_service_account_file(service_account_file, scopes=SCOPES)\n",
    "\n",
    "# Build the service\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "def get_revision_before_timestamp(file_id, timestamp):\n",
    "    try:\n",
    "        # Get the revisions of the file\n",
    "        revisions = drive_service.revisions().list(fileId=file_id).execute()\n",
    "    except Exception as e:\n",
    "        print(f\"File not found: {file_id}\")\n",
    "        return None\n",
    "\n",
    "    # Convert the timestamp to a datetime object\n",
    "    timestamp = pd.to_datetime(timestamp).date()\n",
    "\n",
    "    # Initialize the latest revision before the timestamp as None\n",
    "    latest_revision_before_timestamp = None\n",
    "\n",
    "    for revision in revisions['revisions']:\n",
    "        # Convert the modifiedTime of the revision to a datetime object\n",
    "        modified_time = pd.to_datetime(revision['modifiedTime']).date()\n",
    "        # If the modifiedTime is before the timestamp\n",
    "        if modified_time <= timestamp:\n",
    "            # If this is the first revision or this revision is later than the latest found so far\n",
    "            if latest_revision_before_timestamp is None or modified_time > pd.to_datetime(latest_revision_before_timestamp['modifiedTime']).date():\n",
    "                # Update the latest revision before the timestamp\n",
    "                latest_revision_before_timestamp = revision\n",
    "\n",
    "    return latest_revision_before_timestamp\n",
    "\n",
    "# # Example usage:\n",
    "# file_id = '15AXjZoIhlO9GZl9BTKxSh9JHJkIl_Qh2'\n",
    "# timestamp = '12/20/2023'\n",
    "# timestamp = pd.to_datetime(timestamp).date()\n",
    "# revision = get_revision_before_timestamp(file_id, timestamp)\n",
    "# print(\"revision :\" , revision)\n",
    "\n",
    "# def print_all_revision_ids(file_id):\n",
    "#     try:\n",
    "#         # Get the revisions of the file\n",
    "#         revisions = drive_service.revisions().list(fileId=file_id).execute()\n",
    "#     except Exception as e:\n",
    "#         print(f\"File not found: {file_id}\")\n",
    "#         return None\n",
    "\n",
    "#     # Print all revision ids\n",
    "#     for revision in revisions['revisions']:\n",
    "#         print(revision)\n",
    "\n",
    "# # Example usage:\n",
    "# file_id = '15AXjZoIhlO9GZl9BTKxSh9JHJkIl_Qh2'\n",
    "# print_all_revision_ids(file_id)\n",
    "\n",
    "\n",
    "def get_file_id_from_task_link(task_link):\n",
    "    try:\n",
    "        return task_link.split(\"/\")[-1]\n",
    "    except Exception as e:\n",
    "        print('ERROR' + '='*60)\n",
    "        print(task_link)\n",
    "        return None\n",
    "\n",
    "# Add a new column to the DataFrame for the file IDs\n",
    "selected_rows_df['file_id'] = selected_rows_df['Task Link [Google Colab]'].apply(get_file_id_from_task_link)\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "selected_rows_df['revision'] = selected_rows_df.apply(lambda row: get_revision_before_timestamp(row['file_id'], row['Timestamp']) if row['file_id'] is not None else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_revision_before_timestamp(file_id, timestamp):\n",
    "    try:\n",
    "        # Get the revisions of the file\n",
    "        revisions = drive_service.revisions().list(fileId=file_id).execute()\n",
    "    except Exception as e:\n",
    "        print(f\"File not found: {file_id}\")\n",
    "        return None\n",
    "\n",
    "    # Convert the timestamp to a datetime object\n",
    "    timestamp = pd.to_datetime(timestamp).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
