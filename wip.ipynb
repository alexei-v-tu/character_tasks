{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.llm_api import make_llm_request, LLMAPIFactory\n",
    "import os\n",
    "import sys\n",
    "from project_path import PROJECT_PATH\n",
    "sys.path.insert(0, PROJECT_PATH)\n",
    "\n",
    "\n",
    "service_account_path = PROJECT_PATH + \"/creds/google__sa.json\"\n",
    "tracking_sheet_id = \"1qBU7Kvuuij2fxbqPxebReKMxWgIBmOIE5Gi4ZuX0j_4\"\n",
    "delivery_sheet_id = \"1eUif5I8xhHU8fY0X9v8r2JI9hWPh7Dq_9VXpSIHwww4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have feelings or experiences, but I'm here and ready to assist you with any questions or tasks you have. How can I help you today?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_client = LLMAPIFactory().get()\n",
    "make_llm_request(\n",
    "    llm_client,\n",
    "    [{'role': 'system', 'content': \"How are you today this find evening?\"}],\n",
    "    'gpt-4-1106-preview',\n",
    "    temperature= 0.0,\n",
    "    max_tokens = 4000,\n",
    "    response_format = None,\n",
    "    retries = 3,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for all models: $0.001\n"
     ]
    }
   ],
   "source": [
    "from src.llm_reviewer.llm_api import global_usage_manager\n",
    "\n",
    "global_usage_manager.print_costs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_usage_manager.reset_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function src.llm_reviewer.llm_api.GlobalUsageManager.__init__.<locals>.<lambda>()>,\n",
       "            {'gpt-4-1106-preview': defaultdict(int,\n",
       "                         {'completion_tokens': 36,\n",
       "                          'prompt_tokens': 15,\n",
       "                          'total_tokens': 51})})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_usage_manager._usage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00261"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "77*0.03/1000+30*0.01/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saya/chario/upstream_character_tasks/.venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from src.llm_reviewer.notebook_parser import predict_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling out missing header...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Assistant', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_role([{'role': 'User', 'content': 'how is my favorite GPT doing today?'}, {'role':  '', 'content': 'I am fine thank you'}, {'role':'User', 'content': 'boring as always'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saya/chario/upstream_character_tasks/.venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'src.llm_reviewer.notebook_reviewer' from '/home/saya/chario/upstream_character_tasks/src/llm_reviewer/notebook_reviewer.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from importlib import reload\n",
    "from src.llm_reviewer import notebook_reviewer\n",
    "reload(notebook_reviewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews done: 0, Reviews left after this one: 19\n",
      "Reviews done: 0, Reviews left after this one: 18\n",
      "Reviews done: 0, Reviews left after this one: 17\n",
      "Reviews done: 0, Reviews left after this one: 16\n",
      "Reviews done: 0, Reviews left after this one: 15\n",
      "Reviews done: 0, Reviews left after this one: 14\n",
      "Reviews done: 0, Reviews left after this one: 13\n",
      "Reviews done: 0, Reviews left after this one: 12\n",
      "Reviews done: 0, Reviews left after this one: 11\n",
      "Reviews done: 0, Reviews left after this one: 10\n",
      "Review for turn_id=3 by reviewer='code_reviewer' is done. 1 / 20 reviews completed.\n",
      "Reviews done: 1, Reviews left after this one: 9\n",
      "Review for turn_id=1 by reviewer='code_reviewer' is done. 2 / 20 reviews completed.\n",
      "Reviews done: 2, Reviews left after this one: 8\n",
      "Review for turn_id=1 by reviewer='english_reviewer' is done. 3 / 20 reviews completed.\n",
      "Reviews done: 3, Reviews left after this one: 7\n",
      "Review for turn_id=3 by reviewer='english_reviewer' is done. 4 / 20 reviews completed.\n",
      "Reviews done: 4, Reviews left after this one: 6\n",
      "Review for turn_id=4 by reviewer='code_reviewer' is done. 5 / 20 reviews completed.\n",
      "Reviews done: 5, Reviews left after this one: 5\n",
      "Review for turn_id=2 by reviewer='english_reviewer' is done. 6 / 20 reviews completed.\n",
      "Reviews done: 6, Reviews left after this one: 4\n",
      "Review for turn_id=0 by reviewer='english_reviewer' is done. 7 / 20 reviews completed.\n",
      "Reviews done: 7, Reviews left after this one: 3\n",
      "Review for turn_id=5 by reviewer='english_reviewer' is done. 8 / 20 reviews completed.\n",
      "Reviews done: 8, Reviews left after this one: 2\n",
      "Review for turn_id=2 by reviewer='code_reviewer' is done. 9 / 20 reviews completed.\n",
      "Reviews done: 9, Reviews left after this one: 1\n",
      "Review for turn_id=4 by reviewer='english_reviewer' is done. 10 / 20 reviews completed.\n",
      "Reviews done: 10, Reviews left after this one: 0\n",
      "Review for turn_id=0 by reviewer='code_reviewer' is done. 11 / 20 reviews completed.\n",
      "Review for turn_id=5 by reviewer='code_reviewer' is done. 12 / 20 reviews completed.\n",
      "Review for turn_id=9 by reviewer='code_reviewer' is done. 13 / 20 reviews completed.\n",
      "Review for turn_id=7 by reviewer='english_reviewer' is done. 14 / 20 reviews completed.\n",
      "Review for turn_id=8 by reviewer='english_reviewer' is done. 15 / 20 reviews completed.\n",
      "Review for turn_id=6 by reviewer='english_reviewer' is done. 16 / 20 reviews completed.\n",
      "Review for turn_id=6 by reviewer='code_reviewer' is done. 17 / 20 reviews completed.\n",
      "Review for turn_id=9 by reviewer='english_reviewer' is done. 18 / 20 reviews completed.\n",
      "Review for turn_id=8 by reviewer='code_reviewer' is done. 19 / 20 reviews completed.\n",
      "Review for turn_id=7 by reviewer='code_reviewer' is done. 20 / 20 reviews completed.\n"
     ]
    }
   ],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import review_notebook\n",
    "import nbformat\n",
    "\n",
    "fn = 'src/llm_reviewer/tests/samples/10T__ml_research.ipynb'\n",
    "notebook = nbformat.read(fn, as_version=4)\n",
    "\n",
    "review = review_notebook(\n",
    "    {'nb_parsed_notebook': notebook, 'file_id': fn},\n",
    "    max_threads=10,\n",
    "    progress_counter= None,\n",
    "    verbose = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for all models: $0.333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function src.llm_reviewer.llm_api.GlobalUsageManager.__init__.<locals>.<lambda>()>,\n",
       "            {'gpt-4-1106-preview': defaultdict(int,\n",
       "                         {'completion_tokens': 2555,\n",
       "                          'prompt_tokens': 25642,\n",
       "                          'total_tokens': 28197})})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_usage_manager.print_costs()\n",
    "global_usage_manager._usage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.turn_reviewer import load_rubric, load_system_prompt\n",
    "\n",
    "\n",
    "CODE_PROMPT = load_system_prompt('aggregated_reviews_code_judge')\n",
    "CODE_RUBRICS = load_rubric('code_reviewer')\n",
    "LANG_PROMPT = load_system_prompt('aggregated_reviews_english_judge')\n",
    "LANG_RUBRICS = load_rubric('english_reviewer')\n",
    "\n",
    "\n",
    "from src.llm_reviewer.llm_api import make_llm_request, LLMAPIFactory\n",
    "\n",
    "def get_judgment(reviewer, feedback):\n",
    "    llm_client = LLMAPIFactory().get()\n",
    "    if reviewer == 'code':\n",
    "        prompt = CODE_PROMPT\n",
    "        rubrics = CODE_RUBRICS\n",
    "    elif reviewer == 'lang':\n",
    "        prompt = LANG_PROMPT\n",
    "        rubrics = LANG_RUBRICS\n",
    "    else:\n",
    "        raise ValueError(\"Reviewer type not recognized.\")\n",
    "    \n",
    "    formatted_prompt = prompt.format(FEEDBACK=feedback, GRADING_RUBRICS=rubrics)\n",
    "    judgment = make_llm_request(\n",
    "        llm_client,\n",
    "        [{'role': 'system', 'content': formatted_prompt}],\n",
    "        'gpt-4-1106-preview',\n",
    "        temperature= 0.0,\n",
    "        max_tokens = 4000,\n",
    "        response_format = {'type': \"json_object\"},\n",
    "        retries = 3,\n",
    "    )\n",
    "    return judgment\n",
    "\n",
    "# Example of running the function\n",
    "#judgment = get_judgment(CODE_PROMPT, gpt_reviews_df.iloc[0]['code_feedback'])\n",
    "#judgment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import notebook_reviews_to_df, IssueLevel\n",
    "\n",
    "df = notebook_reviews_to_df(filter(None, [review]), IssueLevel.MEDIUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_path</th>\n",
       "      <th>code_score</th>\n",
       "      <th>lang_score</th>\n",
       "      <th>comb_feedback</th>\n",
       "      <th>code_feedback</th>\n",
       "      <th>lang_feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>src/llm_reviewer/tests/samples/10T__ml_researc...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>#Turn 1:\\n\\n## Language(4/5):\\nNone\\n\\n## Code...</td>\n",
       "      <td>#Turn 1:\\n\\n## Code(4/5):\\n**Medium_Issues**\\n...</td>\n",
       "      <td>#Turn 1:\\n\\n## Language(4/5):\\nNone\\n\\n======\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             nb_path  code_score  lang_score  \\\n",
       "0  src/llm_reviewer/tests/samples/10T__ml_researc...         3.5         4.0   \n",
       "\n",
       "                                       comb_feedback  \\\n",
       "0  #Turn 1:\\n\\n## Language(4/5):\\nNone\\n\\n## Code...   \n",
       "\n",
       "                                       code_feedback  \\\n",
       "0  #Turn 1:\\n\\n## Code(4/5):\\n**Medium_Issues**\\n...   \n",
       "\n",
       "                                       lang_feedback  \n",
       "0  #Turn 1:\\n\\n## Language(4/5):\\nNone\\n\\n======\\...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'judgment': 'The code provided across turns generally functions correctly but often lacks efficiency, error handling, and proper validation, leading to critical issues that need addressing.',\n",
       " 'score': '3'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_judgment('code', df.iloc[0]['code_feedback'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for all models: $0.345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function src.llm_reviewer.llm_api.GlobalUsageManager.__init__.<locals>.<lambda>()>,\n",
       "            {'gpt-4-1106-preview': defaultdict(int,\n",
       "                         {'completion_tokens': 2594,\n",
       "                          'prompt_tokens': 26731,\n",
       "                          'total_tokens': 29325})})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_usage_manager.print_costs()\n",
    "global_usage_manager._usage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034499999999999996"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.345/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.345*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0333"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.333/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.520000000000003"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.0333*4+0.012)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 0.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for all models: $0.345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function src.llm_reviewer.llm_api.GlobalUsageManager.__init__.<locals>.<lambda>()>,\n",
       "            {'gpt-4-1106-preview': defaultdict(int,\n",
       "                         {'completion_tokens': 2594,\n",
       "                          'prompt_tokens': 26731,\n",
       "                          'total_tokens': 29325})})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_usage_manager.print_costs()\n",
    "global_usage_manager._usage_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
