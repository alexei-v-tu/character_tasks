{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.llm_api import make_llm_request, LLMAPIFactory\n",
    "import os\n",
    "import sys\n",
    "from project_path import PROJECT_PATH\n",
    "sys.path.insert(0, PROJECT_PATH)\n",
    "\n",
    "\n",
    "service_account_path = PROJECT_PATH + \"/creds/google__sa.json\"\n",
    "tracking_sheet_id = \"1qBU7Kvuuij2fxbqPxebReKMxWgIBmOIE5Gi4ZuX0j_4\"\n",
    "delivery_sheet_id = \"1eUif5I8xhHU8fY0X9v8r2JI9hWPh7Dq_9VXpSIHwww4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI language model, I don't have feelings or experiences, but I'm here and ready to assist you with any questions or tasks you have. How can I help you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    llm_client = LLMAPIFactory().get()\n",
    "    make_llm_request(\n",
    "        llm_client,\n",
    "        [{'role': 'system', 'content': \"How are you today this find evening?\"}],\n",
    "        'gpt-4-1106-preview',\n",
    "        temperature= 0.0,\n",
    "        max_tokens = 4000,\n",
    "        response_format = None,\n",
    "        retries = 3,\n",
    "        seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saya/chario/upstream_character_tasks/.venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from src.llm_reviewer.notebook_parser import predict_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling out missing header...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Assistant', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_role([{'role': 'User', 'content': 'how is my favorite GPT doing today?'}, {'role':  '', 'content': 'I am fine thank you'}, {'role':'User', 'content': 'boring as always'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.llm_reviewer.notebook_reviewer' from '/home/saya/chario/upstream_character_tasks/src/llm_reviewer/notebook_reviewer.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from importlib import reload\n",
    "from src.llm_reviewer import notebook_reviewer\n",
    "reload(notebook_reviewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews done: 0, Reviews left after this one: 19\n",
      "Reviews done: 0, Reviews left after this one: 18\n",
      "Reviews done: 0, Reviews left after this one: 17\n",
      "Reviews done: 0, Reviews left after this one: 16\n",
      "Reviews done: 0, Reviews left after this one: 15\n",
      "Reviews done: 0, Reviews left after this one: 14\n",
      "Reviews done: 0, Reviews left after this one: 13\n",
      "Reviews done: 0, Reviews left after this one: 12\n",
      "Reviews done: 0, Reviews left after this one: 11\n",
      "Reviews done: 0, Reviews left after this one: 10\n",
      "Review for turn_id=3 by reviewer='code_reviewer' is done. 1 / 20 reviews completed.\n",
      "Reviews done: 1, Reviews left after this one: 9\n",
      "Review for turn_id=1 by reviewer='code_reviewer' is done. 2 / 20 reviews completed.\n",
      "Reviews done: 2, Reviews left after this one: 8\n",
      "Review for turn_id=1 by reviewer='english_reviewer' is done. 3 / 20 reviews completed.\n",
      "Reviews done: 3, Reviews left after this one: 7\n",
      "Review for turn_id=3 by reviewer='english_reviewer' is done. 4 / 20 reviews completed.\n",
      "Reviews done: 4, Reviews left after this one: 6\n",
      "Review for turn_id=4 by reviewer='code_reviewer' is done. 5 / 20 reviews completed.\n",
      "Reviews done: 5, Reviews left after this one: 5\n",
      "Review for turn_id=5 by reviewer='english_reviewer' is done. 6 / 20 reviews completed.\n",
      "Reviews done: 6, Reviews left after this one: 4\n",
      "Review for turn_id=0 by reviewer='english_reviewer' is done. 7 / 20 reviews completed.\n",
      "Reviews done: 7, Reviews left after this one: 3\n",
      "Review for turn_id=4 by reviewer='english_reviewer' is done. 8 / 20 reviews completed.\n",
      "Reviews done: 8, Reviews left after this one: 2\n",
      "Review for turn_id=2 by reviewer='code_reviewer' is done. 9 / 20 reviews completed.\n",
      "Reviews done: 9, Reviews left after this one: 1\n",
      "Review for turn_id=2 by reviewer='english_reviewer' is done. 10 / 20 reviews completed.\n",
      "Reviews done: 10, Reviews left after this one: 0\n",
      "Review for turn_id=6 by reviewer='code_reviewer' is done. 11 / 20 reviews completed.\n",
      "Review for turn_id=7 by reviewer='english_reviewer' is done. 12 / 20 reviews completed.\n",
      "Review for turn_id=6 by reviewer='english_reviewer' is done. 13 / 20 reviews completed.\n",
      "Review for turn_id=5 by reviewer='code_reviewer' is done. 14 / 20 reviews completed.\n",
      "Review for turn_id=0 by reviewer='code_reviewer' is done. 15 / 20 reviews completed.\n",
      "Review for turn_id=9 by reviewer='code_reviewer' is done. 16 / 20 reviews completed.\n",
      "Review for turn_id=8 by reviewer='english_reviewer' is done. 17 / 20 reviews completed.\n",
      "Review for turn_id=9 by reviewer='english_reviewer' is done. 18 / 20 reviews completed.\n",
      "Review for turn_id=7 by reviewer='code_reviewer' is done. 19 / 20 reviews completed.\n",
      "Review for turn_id=8 by reviewer='code_reviewer' is done. 20 / 20 reviews completed.\n"
     ]
    }
   ],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import review_notebook\n",
    "import nbformat\n",
    "\n",
    "fn = 'src/llm_reviewer/tests/samples/10T__ml_research.ipynb'\n",
    "notebook = nbformat.read(fn, as_version=4)\n",
    "\n",
    "review = review_notebook(\n",
    "    {'nb_parsed_notebook': notebook, 'file_id': fn},\n",
    "    max_threads=10,\n",
    "    progress_counter= None,\n",
    "    verbose = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'turns': [{'turn': [{'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': \"Do you know the iris dataset for ml? I wanted to explore scikit learn algorithms on it. Give me the code to solve it using some algo from scikit learn. I'm in an empty folder right now. linux\"}]},\n",
       "    {'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Yes, I know the iris dataset that is commonly used as a toy dataset to showcase the work of machine learning classification algorithms.\\n\\nscikit-learn has multiple algorithms that are capable of \"solving\" it, let\\'s start with a simple logistic regression.\\n\\nBut first, to start we will need to set up an environment with scikit-learn python package installed.\\n\\nAssuming that you are in a project root directory, let\\'s set up a virtual environment for this task.\\n\\nRun the following in the command line:\\n\\n```\\npython -m venv venv\\n```\\n\\nThis will create a folder named venv with the virtual environment for you.\\n\\nNext, you will need to activate it and install the scikit-learn package using pip\\n\\n```\\nsource venv/bin/activate && pip install scikit-learn\\n```\\n\\nNow with all this out of the way, here\\'s the code to actually train an algorithm to solve this dataset. \\n\\niris_tests.py\\n```\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\nlogreg = LogisticRegression(max_iter=200)\\n\\nlogreg.fit(X_train, y_train)\\n\\ny_pred = logreg.predict(X_test)\\n\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(\"Classification accuracy using logistic regression:\", accuracy)\\n```\\n\\nAll you have to do is `python iris_tests.py`'}]}],\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': \"The assistant's response could be more concise by eliminating unnecessary explanations about the virtual environment setup, as the user's request was specifically for code to solve the iris dataset problem using scikit-learn.\",\n",
       "     'minor_issues': \"The phrase 'to actually train an algorithm to solve this dataset' could be more concise. Suggested revision: 'to train a logistic regression model on the iris dataset.'\",\n",
       "     'summary': \"The response could be more concise by focusing on the user's request for code, rather than including environment setup instructions.\"},\n",
       "    'score': 4},\n",
       "   'code_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': 'The code provided assumes the user has Python and the necessary permissions to create a virtual environment and install packages, which may not always be the case.',\n",
       "     'minor_issues': 'The code snippet does not include instructions on how to exit the virtual environment once the user is done. It would be helpful to include a comment or instruction on deactivating the virtual environment.\\nThe file name `iris_tests.py` implies that it contains tests, which might be misleading. A more appropriate name like `iris_classification.py` could be used to reflect the content of the script.',\n",
       "     'summary': \"The code is correct and follows best practices, but it could be improved by considering the user's environment setup and providing complete instructions for managing the virtual environment.\"},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'I got 1.0. That was pretty good. Why 42 though?'}]},\n",
       "    {'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Just keeping in with the tradition - 42 is a humorous number from the book The Hitchhiker\\'s Guide to the Galaxy by Douglas Adams. In the book, the number was the answer to \"the Ultimate Question of Life, the Universe, and Everything\" provided by a supercomputer when it was asked to answer the ultimate question. So, it\\'s an inside joke for the programmers to use it in various places, often as a seed for a random number generator.'}]}],\n",
       "   'code_review': {'feedback_text': {}, 'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': 'The explanation could be more concise by eliminating unnecessary details such as the description of the supercomputer.',\n",
       "     'summary': 'The reply is clear and relevant but could be slightly more concise.'},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'lol. Alright.\\n\\nBack to the scikit, now that I know it works, show me multiple algos to solve it at once.'}]},\n",
       "    {'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'The package has quite a few ml algorithms that would work on this dataset. Let\\'s use some of them:\\n\\nCreate a new file called `many_classifiers_on_iris.py`\\n\\nThe code:\\n\\n```\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.preprocessing import StandardScaler\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Standardize features by removing the mean and scaling to unit variance for the algorithms that benefit from it\\nscaler = StandardScaler()\\nscaled_X_train = scaler.fit_transform(X_train)\\nscaled_X_test = scaler.transform(X_test)\\n\\n\\n# First we will gather all the algorithms into a single array as scikit provides a universal API to work with the algorithms - very convenient\\nalgorithms = []\\n\\n# Initialize Logistic Regression as the last time\\nlogreg = LogisticRegression(max_iter=200)\\nalgorithms.append({\\'algo\\': logreg, \\'scale\\': True})\\n\\n# Initialize the K-Nearest Neighbors classifier with n_neighbors=3\\nknn = KNeighborsClassifier(n_neighbors=3)\\nalgorithms.append({\\'algo\\': knn, \\'scale\\': True})\\n\\n# Create a SVM Classifier\\nsvm_classifier = SVC(kernel=\\'linear\\', random_state=42)\\nalgorithms.append({\\'algo\\': svm_classifier, \\'scale\\': True})\\n\\n# Creating a Decision Tree Classifier\\nclf = DecisionTreeClassifier()\\nalgorithms.append({\\'algo\\': clf, \\'scale\\': False})\\n\\n# Initialize RandomForestClassifier\\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\\nalgorithms.append({\\'algo\\': rf_classifier, \\'scale\\': False})\\n\\n# Initialize GradientBoostingClassifier\\ngb_clf = GradientBoostingClassifier(random_state=42)\\nalgorithms.append({\\'algo\\': gb_clf, \\'scale\\': True})\\n\\n# Creating a Gaussian Naive Bayes model\\ngnb = GaussianNB()\\nalgorithms.append({\\'algo\\': gnb, \\'scale\\': False})\\n\\n# Now we can loop over the algorithms list and apply each one\\nfor algorithm in algorithms:\\n    current_x = X_train\\n    current_x_test = X_test\\n    \\n    if algorithm[\\'scale\\']:\\n        current_x = scaled_X_train\\n        current_x_test = scaled_X_test\\n    \\n    algorithm[\\'algo\\'].fit(current_x, y_train)\\n    y_pred = algorithm[\\'algo\\'].predict(current_x_test)\\n    accuracy = accuracy_score(y_test, y_pred)\\n    print(\"Classification accuracy using\", type(algorithm[\\'algo\\']).__name__, \":\", accuracy)\\n\\n```\\n\\nYou can now run `python many_classifiers_on_iris.py` and see the accuracy printed for each one of them.'}]}],\n",
       "   'code_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': 'The code could be refactored to avoid repetition and improve maintainability. The use of a dictionary to store classifiers and their scaling needs is a good approach, but the loop could be simplified by directly iterating over the classifiers and their associated scale flag.',\n",
       "     'minor_issues': \"The variable names `current_x` and `current_x_test` could be more descriptive. It would be better to use names like `train_features` and `test_features` to make the code more readable. Additionally, the comment about initializing Logistic Regression 'as the last time' assumes prior context which may not be clear to a new reader or in a standalone script.\",\n",
       "     'summary': 'The code is correct and functional but could be improved in terms of maintainability and readability.'},\n",
       "    'score': 4},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The explanation preceding the code could be more concise. The phrase 'First we will gather all the algorithms into a single array as scikit provides a universal API to work with the algorithms - very convenient' could be shortened to 'First, we'll gather all algorithms into an array, leveraging scikit-learn's universal API.'\",\n",
       "     'summary': \"The assistant's reply is clear, precise, and relevant, with a minor issue regarding conciseness in the introductory explanation.\"},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Got 1.0 for almost all of them. Do you know why SVM is called SVC not SVMC?'}]},\n",
       "    {'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'In SVM or Support Vector Machine, \"machine\" just means it\\'s a system that learns from data to make predictions. When SVM is used for sorting data into categories, people call it \"Support Vector Classification\" or SVC in scikit-learn. This name change simply points out that the SVM approach is specifically being used for classification.'}]}],\n",
       "   'code_review': {'feedback_text': {}, 'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The explanation could be more concise by removing unnecessary words without losing meaning. For example, 'just means it's a system that learns from data to make predictions' could be shortened to 'is a system that learns from data for predictions.'\",\n",
       "     'summary': 'The reply is clear, relevant, and free of mistakes, but it could be slightly more concise.'},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': \"I'm wondering about about a more difficult dataset, do you have any in mind that would worsen the results? datasets from scikit datasets\"}]},\n",
       "    {'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'To find a more challenging dataset for the classifiers we have, we can move them into a separate module and call it like a function for a given dataset. After testing it on a few datasets, we can find a more challenging dataset for the algorithms we have given the same hyperparameters.\\n\\nCreate a file classifiers_set.py\\n\\n```\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import train_test_split\\n\\ndef run_classifiers(X, y):\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    # Standardize features by removing the mean and scaling to unit variance for the algorithms that benefit from it\\n    scaler = StandardScaler()\\n    scaled_X_train = scaler.fit_transform(X_train)\\n    scaled_X_test = scaler.transform(X_test)\\n\\n    # Gather all the algorithms into a single array\\n    algorithms = []\\n\\n    # Initialize Logistic Regression as the last time\\n    logreg = LogisticRegression(max_iter=200)\\n    algorithms.append({\\'algo\\': logreg, \\'scale\\': True})\\n\\n    # Initialize the K-Nearest Neighbors classifier with n_neighbors=3\\n    knn = KNeighborsClassifier(n_neighbors=3)\\n    algorithms.append({\\'algo\\': knn, \\'scale\\': True})\\n\\n    # Create a SVM Classifier\\n    svm_classifier = SVC(kernel=\\'linear\\')\\n    algorithms.append({\\'algo\\': svm_classifier, \\'scale\\': True})\\n\\n    # Creating a Decision Tree Classifier\\n    clf = DecisionTreeClassifier()\\n    algorithms.append({\\'algo\\': clf, \\'scale\\': False})\\n\\n    # Initialize RandomForestClassifier\\n    rf_classifier = RandomForestClassifier(n_estimators=100)\\n    algorithms.append({\\'algo\\': rf_classifier, \\'scale\\': False})\\n\\n    # Initialize GradientBoostingClassifier\\n    gb_clf = GradientBoostingClassifier()\\n    algorithms.append({\\'algo\\': gb_clf, \\'scale\\': True})\\n\\n    # Creating a Gaussian Naive Bayes model\\n    gnb = GaussianNB()\\n    algorithms.append({\\'algo\\': gnb, \\'scale\\': False})\\n\\n    results = []\\n\\n    # Loop over the algorithms list and apply each one\\n    for algorithm in algorithms:\\n        current_x = X_train\\n        current_x_test = X_test\\n\\n        if algorithm[\\'scale\\']:\\n            current_x = scaled_X_train\\n            current_x_test = scaled_X_test\\n\\n        algorithm[\\'algo\\'].fit(current_x, y_train)\\n        y_pred = algorithm[\\'algo\\'].predict(current_x_test)\\n        accuracy = accuracy_score(y_test, y_pred)\\n        results.append({\\'classifier\\': type(algorithm[\\'algo\\']).__name__, \\'accuracy\\': accuracy})\\n\\n    return results\\n\\n```\\n\\nAnd to test multiple datasets let\\'s create a file `test_multiple_datasets.py`\\n\\n```\\nfrom sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\\nfrom classifiers_set import run_classifiers\\n\\ndatasets = [load_iris, load_digits, load_wine, load_breast_cancer]\\n\\nfor dataset in datasets:\\n    X, y = dataset(return_X_y=True)\\n    results = run_classifiers(X, y)\\n    print(f\"Results for {dataset.__name__}:\")\\n    for result in results:\\n        print(f\"Classifier: {result[\\'classifier\\']}, Accuracy: {result[\\'accuracy\\']}\")\\n\\n\\n```\\n\\nTo check the results: `python test_multiple_datasets.py`'}]}],\n",
       "   'code_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': 'The code lacks exception handling for potential issues that may arise when loading datasets or running classifiers. Adding try-except blocks would improve robustness.',\n",
       "     'minor_issues': 'The code could be more modular by separating the logic for scaling and fitting into separate functions. This would improve readability and maintainability.',\n",
       "     'summary': 'The code is correct and functional but could be improved in terms of robustness and modularity.'},\n",
       "    'score': 4},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': \"The assistant's reply could be more directly aligned with the user's request for a 'more difficult dataset'. The user's interest is in datasets that could potentially worsen the results, but the assistant's response focuses on the process of testing classifiers on various datasets without specifying which might be more challenging or why.\",\n",
       "     'minor_issues': 'The response could be more concise by summarizing the purpose of the provided code before diving into the specifics. This would help the user understand the context and intention behind the code snippets.',\n",
       "     'summary': \"The assistant's response is clear and mostly well-structured but lacks direct engagement with the user's specific interest in more challenging datasets and could benefit from a concise introduction.\"},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': \"Huh. These basic classifiers tore through all the datasets you've tried. I think what would be interesting to do now is to get back to our iris dataset and see how the classifiers handle noise in the dataset. Add 5% disturbance for each feature.\"}]},\n",
       "    {'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Interesting result and a great idea on how to explore these classifiers more!\\nTo add noise to the training set of a dataset and run through the classifiers we have you will need to modify the run classifiers function:\\n\\nReplace\\n```\\ndef run_classifiers(X, y):\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n```\\nwith\\n```\\ndef run_classifiers(X, y, X_test=None, y_test=None):\\n    if X_test is None:\\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    else:\\n        X_train, y_train = X, y\\n```\\nWith this you will be able to provide run_classifiers with the modified training data and unmodified test data.\\n\\nNow create a file `test_with_noise.py`\\n\\n```\\nfrom sklearn.datasets import load_iris\\nfrom classifiers_set import run_classifiers\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\n\\ndef add_noise(X_train, noise_level=0.05):\\n    \"\"\"\\n    Function to add uniform random noise to the features in %\\n    \"\"\"\\n    noise = np.random.uniform(-noise_level, noise_level, X_train.shape)\\n    return X_train + X_train*noise\\n\\ndef run_experiment(noise_level):\\n    \"\"\"\\n    Function to run the experiment with the classifiers.\\n    \"\"\"\\n    X, y = load_iris(return_X_y=True)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    X_train_noisy = add_noise(X_train, noise_level)\\n    results = run_classifiers(X_train_noisy, y_train, X_test, y_test)\\n    print(f\"Results for {load_iris.__name__}:\")\\n    for result in results:\\n        print(f\"Classifier: {result[\\'classifier\\']}, Accuracy: {result[\\'accuracy\\']}\")\\n\\nif __name__ == \"__main__\":\\n    run_experiment(0.05)\\n```\\n\\nRun it with `python test_with_noise.py`'}]}],\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': 'The explanation of the code changes and the new `test_with_noise.py` file could be more concise. The instructions on how to run the experiment are clear, but the explanation of the code modifications includes some redundancy that could be streamlined for brevity.',\n",
       "     'summary': \"The assistant's reply is clear, precise, and relevant, with only minor issues related to conciseness.\"},\n",
       "    'score': 4},\n",
       "   'code_review': {'feedback_text': {'critical_issues': 'The code lacks error handling for the case where the noise level is too high, potentially resulting in negative feature values which may not be valid for all classifiers.',\n",
       "     'medium_issues': 'The function `add_noise` could be improved by ensuring that the noise added does not result in negative values for features that are inherently non-negative. The `run_classifiers` function is modified to accept test data, but the original functionality is not preserved for cases where test data is not provided.',\n",
       "     'minor_issues': \"The `print` statement in `run_experiment` could be more informative by including the noise level in the output. The `run_classifiers` function's docstring is not updated to reflect the new parameters and functionality.\",\n",
       "     'summary': 'The critical issue is the lack of error handling for high noise levels, which could lead to invalid data for classifiers.'},\n",
       "    'score': 3}},\n",
       "  {'turn': [{'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Result is still too good. Do a grid search over the noise level to find a tipping point.'}]},\n",
       "    {'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Wonderful idea. Here\\'s the code to look for a noise level that degrades at least one classifier to less than 50% accuracy.\\nI\\'ve split the noise level into 5% increments for this search.\\n\\nAdd ```return results``` to `test_with_noise.py` file at the end of run_experiment function.\\n\\nThen create a file `noise_level_search_on_iris.py`\\n\\n```\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom test_with_noise import run_experiment\\nimport numpy as np\\n\\n\\ndef grid_search_noise_levels():\\n    \"\"\"\\n    Function to perform a grid search over the noise levels.\\n    \"\"\"\\n    noise_levels = np.linspace(0, 1, 21)\\n    for noise_level in noise_levels:\\n        print(\\'-\\'*60)\\n        print(f\"Results for noise level {noise_level}:\")\\n        results = run_experiment(noise_level)\\n        if any(result[\\'accuracy\\'] < 0.5 for result in results):\\n            print(\\'First <0.5 accuracy result\\')\\n            return\\n\\ngrid_search_noise_levels()\\n\\n```\\n\\nRun it with: `python noise_level_search_on_iris.py`'}]}],\n",
       "   'code_review': {'feedback_text': {'critical_issues': \"The code does not capture or display the noise level at which the classifier's accuracy falls below 50%.\",\n",
       "     'medium_issues': 'The function grid_search_noise_levels() prints results but does not store them for further analysis.',\n",
       "     'minor_issues': \"The print statement 'First <0.5 accuracy result' could be more informative by including the noise level and the classifier that fell below 50% accuracy.\"},\n",
       "    'score': 3,\n",
       "    'summary': 'The critical issue is that the code does not fulfill the requirement to identify the noise level where the accuracy drops below 50%, which is the main goal of the grid search.'},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The phrase 'Wonderful idea.' could be omitted for a more professional tone. The explanation 'I've split the noise level into 5% increments for this search.' is redundant as the code snippet clearly shows the noise levels being split into 5% increments through `np.linspace(0, 1, 21)`. The instruction 'Add `return results` to `test_with_noise.py` file at the end of run_experiment function.' could be more concise by directly stating 'Add `return results` at the end of the `run_experiment` function in `test_with_noise.py`.'\",\n",
       "     'summary': 'The reply is clear and relevant, with only minor issues related to conciseness and professional tone.'},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'This is too much information on the screen for me. I want you to show me how to plot this data as a line for each of the classifiers depending on the noise level.'}]},\n",
       "    {'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'To plot the results, you will need to install another package - matplotlib.\\nRun `pip install matplotlib`.\\n\\nNext, replace the `noise_level_search_on_iris.py` with the following code:\\n\\n```\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom test_with_noise import run_experiment\\nimport numpy as np\\n\\n\\ndef grid_search_noise_levels():\\n    \"\"\"\\n    Function to perform a grid search over the noise levels.\\n    \"\"\"\\n    noise_levels = np.linspace(0, 1, 21)\\n    all_results = []\\n    for noise_level in noise_levels:\\n        print(\\'-\\'*60)\\n        print(f\"Results for noise level {noise_level}:\")\\n        results = run_experiment(noise_level)\\n        all_results.append((noise_level, results))\\n        if any(result[\\'accuracy\\'] < 0.5 for result in results):\\n            return all_results\\n    return all_results\\n\\ndef plot_results(all_results):\\n    \"\"\"\\n    Function to plot the results.\\n    \"\"\"\\n    classifier_names = [result[\\'classifier\\'] for result in all_results[0][1]]\\n    noise_levels = [result[0] for result in all_results]\\n    for i, classifier_name in enumerate(classifier_names):\\n        accuracies = [result[1][i][\\'accuracy\\'] for result in all_results]\\n        plt.plot(noise_levels, accuracies, label=classifier_name)\\n    plt.xlabel(\\'Noise level\\')\\n    plt.ylabel(\\'Accuracy\\')\\n    plt.legend()\\n    plt.savefig(\\'results.png\\')\\n\\nall_results = grid_search_noise_levels()\\nplot_results(all_results)\\n```\\n\\nAfter running `python noise_level_search_on_iris.py` you should see `results.png` file in the folder with the chart we created.'}]}],\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The explanation of the code replacement could be more concise. The phrase 'replace the `noise_level_search_on_iris.py` with the following code' could be simplified to 'Replace `noise_level_search_on_iris.py` with:' to streamline the instruction.\",\n",
       "     'summary': \"The assistant's reply is clear, precise, and relevant, with a minor issue regarding conciseness.\"},\n",
       "    'score': 4},\n",
       "   'code_review': {'feedback_text': {'critical_issues': \"The code snippet provided does not include the definition of the 'run_experiment' function, which is critical for the execution of the script. Without this function, the script will fail to run.\",\n",
       "     'medium_issues': \"The code does not handle the case where 'all_results' might be empty before plotting, which could lead to an IndexError when accessing 'all_results[0][1]'.\",\n",
       "     'minor_issues': \"The code lacks comments explaining the logic within the 'grid_search_noise_levels' and 'plot_results' functions, which could improve readability and maintainability.\",\n",
       "     'summary': \"The absence of the 'run_experiment' function definition is a critical issue that prevents the code from running, and the potential IndexError due to unhandled empty 'all_results' is a medium issue affecting robustness.\"},\n",
       "    'score': 2}},\n",
       "  {'turn': [{'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': \"I've generated the image. Give me a cluster bar chart for this thing.\\nBut reduce number of noise points to one per 30%. Make the bars thin as we have lots of classifiers.\"}]},\n",
       "    {'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Sure thing. Add this function to the `noise_level_search_on_iris.py` after `plot_results(all_results)`\\n\\n```\\ndef plot_results_cluster_bar(all_results):\\n    \"\"\"\\n    Function to plot the results.\\n    \"\"\"\\n    classifier_names = [result[\\'classifier\\'] for result in all_results[0][1]]\\n    print(all_results)\\n    noise_levels = [result[0] for result in all_results if result[0] % 0.3 < 0.001]\\n    print(noise_levels)\\n    width = 0.1 / len(classifier_names)  # Adjust bar width based on number of classifiers\\n    fig, ax = plt.subplots()\\n    for i, classifier_name in enumerate(classifier_names):\\n        accuracies = [result[1][i][\\'accuracy\\'] for result in all_results if result[0] % 0.3  < 0.001]\\n        ax.bar([noise + i * width for noise in noise_levels], accuracies, width, label=classifier_name)\\n    ax.set_xlabel(\\'Noise level\\')\\n    ax.set_ylabel(\\'Accuracy\\')\\n    ax.legend()\\n    plt.savefig(\\'results_cluster_bar.png\\')\\n```\\n\\nand then add this line at the end of the file to create the new chart - `plot_results_cluster_bar(all_results)`.\\n\\nThe new image will be named `results_cluster_bar.png`.'}]}],\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The explanation of the function `plot_results_cluster_bar` could be more concise. The comments within the code, such as 'Function to plot the results.', do not add significant value and could be omitted for brevity.\",\n",
       "     'summary': \"The assistant's reply is clear, precise, and relevant, with only a minor issue regarding the conciseness of the code comments.\"},\n",
       "    'score': 4},\n",
       "   'code_review': {'feedback_text': {'critical_issues': 'The function `plot_results_cluster_bar` does not filter the noise levels correctly. The condition `result[0] % 0.3 < 0.001` does not ensure that there is only one noise point per 30%. It may include noise levels that are not exactly multiples of 0.3 due to floating-point precision issues.',\n",
       "     'medium_issues': 'The width calculation for the bars is based on the number of classifiers, which may result in extremely thin bars if there are many classifiers. This could make the chart difficult to read.',\n",
       "     'minor_issues': 'The function `plot_results_cluster_bar` lacks proper documentation. The docstring should explain the expected structure of `all_results` and the purpose of the function more clearly. The print statements used for debugging should be removed in the final version of the code.'},\n",
       "    'score': 3,\n",
       "    'summary': 'The code is correct in terms of syntax but has a critical issue with the noise level filtering logic and medium issues with potential readability due to bar width. Minor improvements are also needed in documentation and code cleanliness.'}},\n",
       "  {'turn': [{'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Thanks! What was the point of doing < 0.001 in the code above instead of == 0 though?'}]},\n",
       "    {'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': \"I've used < 0.001 comparisons instead of == 0 to avoid problems with the float point numbers precision. As we used `np.linspace(0, 1, 21)` to generate the noise levels, we might not get precise values for 0.3 0.6 0.9 even though we've separated [0, 1] into 21 points due to how float numbers are stored in the computer. So to avoid a situation, where a very small difference prevents numbers from being seen as equal, I've added a threshold to ignore these small differences.\"}]}],\n",
       "   'code_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': '',\n",
       "     'summary': \"There is no code to review in the LLM's reply, as it contains only an explanation for a choice made in a previous code snippet.\"},\n",
       "    'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The explanation could be more concise by eliminating redundancy. For instance, 'due to how float numbers are stored in the computer' could be shortened to 'due to float number storage limitations'.\",\n",
       "     'summary': 'The reply is clear and relevant, with only a minor issue regarding conciseness.'},\n",
       "    'score': 4}}],\n",
       " 'nb_path': 'src/llm_reviewer/tests/samples/10T__ml_research.ipynb'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'turns': [{'turn': [{'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': \"Do you know the iris dataset for ml? I wanted to explore scikit learn algorithms on it. Give me the code to solve it using some algo from scikit learn. I'm in an empty folder right now. linux\"}]},\n",
       "    {'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Yes, I know the iris dataset that is commonly used as a toy dataset to showcase the work of machine learning classification algorithms.\\n\\nscikit-learn has multiple algorithms that are capable of \"solving\" it, let\\'s start with a simple logistic regression.\\n\\nBut first, to start we will need to set up an environment with scikit-learn python package installed.\\n\\nAssuming that you are in a project root directory, let\\'s set up a virtual environment for this task.\\n\\nRun the following in the command line:\\n\\n```\\npython -m venv venv\\n```\\n\\nThis will create a folder named venv with the virtual environment for you.\\n\\nNext, you will need to activate it and install the scikit-learn package using pip\\n\\n```\\nsource venv/bin/activate && pip install scikit-learn\\n```\\n\\nNow with all this out of the way, here\\'s the code to actually train an algorithm to solve this dataset. \\n\\niris_tests.py\\n```\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\nlogreg = LogisticRegression(max_iter=200)\\n\\nlogreg.fit(X_train, y_train)\\n\\ny_pred = logreg.predict(X_test)\\n\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(\"Classification accuracy using logistic regression:\", accuracy)\\n```\\n\\nAll you have to do is `python iris_tests.py`'}]}],\n",
       "   'code_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': '',\n",
       "     'summary': 'There is no code to review in the LLM_REPLY as it does not contain any code snippets or code cells.'},\n",
       "    'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': \"The assistant's reply lacks the actual content requested by the user. The user asked for code to solve the iris dataset using a scikit-learn algorithm, but the assistant failed to provide any code or further instructions.\",\n",
       "     'minor_issues': '',\n",
       "     'summary': \"The assistant's response is missing the requested code and instructions, which is a medium issue as it does not fulfill the user's request.\"},\n",
       "    'score': 3}},\n",
       "  {'turn': [{'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'I got 1.0. That was pretty good. Why 42 though?'}]},\n",
       "    {'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Just keeping in with the tradition - 42 is a humorous number from the book The Hitchhiker\\'s Guide to the Galaxy by Douglas Adams. In the book, the number was the answer to \"the Ultimate Question of Life, the Universe, and Everything\" provided by a supercomputer when it was asked to answer the ultimate question. So, it\\'s an inside joke for the programmers to use it in various places, often as a seed for a random number generator.'}]}],\n",
       "   'code_review': {'feedback_text': {}, 'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The reply lacks context and does not address the user's score of 1.0 or the significance of the number 42, which may confuse readers unfamiliar with the reference.\",\n",
       "     'summary': \"The reply is clear and free of language errors but misses the opportunity to provide context or acknowledge the user's score, which affects relevance and clarity.\"},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'lol. Alright.\\n\\nBack to the scikit, now that I know it works, show me multiple algos to solve it at once.'}]},\n",
       "    {'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'The package has quite a few ml algorithms that would work on this dataset. Let\\'s use some of them:\\n\\nCreate a new file called `many_classifiers_on_iris.py`\\n\\nThe code:\\n\\n```\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.preprocessing import StandardScaler\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Standardize features by removing the mean and scaling to unit variance for the algorithms that benefit from it\\nscaler = StandardScaler()\\nscaled_X_train = scaler.fit_transform(X_train)\\nscaled_X_test = scaler.transform(X_test)\\n\\n\\n# First we will gather all the algorithms into a single array as scikit provides a universal API to work with the algorithms - very convenient\\nalgorithms = []\\n\\n# Initialize Logistic Regression as the last time\\nlogreg = LogisticRegression(max_iter=200)\\nalgorithms.append({\\'algo\\': logreg, \\'scale\\': True})\\n\\n# Initialize the K-Nearest Neighbors classifier with n_neighbors=3\\nknn = KNeighborsClassifier(n_neighbors=3)\\nalgorithms.append({\\'algo\\': knn, \\'scale\\': True})\\n\\n# Create a SVM Classifier\\nsvm_classifier = SVC(kernel=\\'linear\\', random_state=42)\\nalgorithms.append({\\'algo\\': svm_classifier, \\'scale\\': True})\\n\\n# Creating a Decision Tree Classifier\\nclf = DecisionTreeClassifier()\\nalgorithms.append({\\'algo\\': clf, \\'scale\\': False})\\n\\n# Initialize RandomForestClassifier\\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\\nalgorithms.append({\\'algo\\': rf_classifier, \\'scale\\': False})\\n\\n# Initialize GradientBoostingClassifier\\ngb_clf = GradientBoostingClassifier(random_state=42)\\nalgorithms.append({\\'algo\\': gb_clf, \\'scale\\': True})\\n\\n# Creating a Gaussian Naive Bayes model\\ngnb = GaussianNB()\\nalgorithms.append({\\'algo\\': gnb, \\'scale\\': False})\\n\\n# Now we can loop over the algorithms list and apply each one\\nfor algorithm in algorithms:\\n    current_x = X_train\\n    current_x_test = X_test\\n    \\n    if algorithm[\\'scale\\']:\\n        current_x = scaled_X_train\\n        current_x_test = scaled_X_test\\n    \\n    algorithm[\\'algo\\'].fit(current_x, y_train)\\n    y_pred = algorithm[\\'algo\\'].predict(current_x_test)\\n    accuracy = accuracy_score(y_test, y_pred)\\n    print(\"Classification accuracy using\", type(algorithm[\\'algo\\']).__name__, \":\", accuracy)\\n\\n```\\n\\nYou can now run `python many_classifiers_on_iris.py` and see the accuracy printed for each one of them.'}]}],\n",
       "   'code_review': {'feedback_text': {'critical_issues': 'The code snippet for review is not present in the LLM_REPLY.',\n",
       "     'summary': 'The LLM_REPLY does not contain any code to review.'},\n",
       "    'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The response 'lol. Alright.' may be perceived as informal and not aligned with professional communication standards expected from an AI assistant.\",\n",
       "     'summary': \"The assistant's reply is clear and relevant but uses informal language that slightly deviates from professional standards.\"},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Got 1.0 for almost all of them. Do you know why SVM is called SVC not SVMC?'}]},\n",
       "    {'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'In SVM or Support Vector Machine, \"machine\" just means it\\'s a system that learns from data to make predictions. When SVM is used for sorting data into categories, people call it \"Support Vector Classification\" or SVC in scikit-learn. This name change simply points out that the SVM approach is specifically being used for classification.'}]}],\n",
       "   'code_review': {'feedback_text': {}, 'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The response could be enhanced by providing a brief explanation of why SVM is called SVC in the context of scikit-learn, which would add clarity and relevance to the user's question.\",\n",
       "     'summary': \"The response is clear and correct but lacks a brief explanation that would make it more informative and relevant to the user's query.\"},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': \"I'm wondering about about a more difficult dataset, do you have any in mind that would worsen the results? datasets from scikit datasets\"}]},\n",
       "    {'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'To find a more challenging dataset for the classifiers we have, we can move them into a separate module and call it like a function for a given dataset. After testing it on a few datasets, we can find a more challenging dataset for the algorithms we have given the same hyperparameters.\\n\\nCreate a file classifiers_set.py\\n\\n```\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import train_test_split\\n\\ndef run_classifiers(X, y):\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    # Standardize features by removing the mean and scaling to unit variance for the algorithms that benefit from it\\n    scaler = StandardScaler()\\n    scaled_X_train = scaler.fit_transform(X_train)\\n    scaled_X_test = scaler.transform(X_test)\\n\\n    # Gather all the algorithms into a single array\\n    algorithms = []\\n\\n    # Initialize Logistic Regression as the last time\\n    logreg = LogisticRegression(max_iter=200)\\n    algorithms.append({\\'algo\\': logreg, \\'scale\\': True})\\n\\n    # Initialize the K-Nearest Neighbors classifier with n_neighbors=3\\n    knn = KNeighborsClassifier(n_neighbors=3)\\n    algorithms.append({\\'algo\\': knn, \\'scale\\': True})\\n\\n    # Create a SVM Classifier\\n    svm_classifier = SVC(kernel=\\'linear\\')\\n    algorithms.append({\\'algo\\': svm_classifier, \\'scale\\': True})\\n\\n    # Creating a Decision Tree Classifier\\n    clf = DecisionTreeClassifier()\\n    algorithms.append({\\'algo\\': clf, \\'scale\\': False})\\n\\n    # Initialize RandomForestClassifier\\n    rf_classifier = RandomForestClassifier(n_estimators=100)\\n    algorithms.append({\\'algo\\': rf_classifier, \\'scale\\': False})\\n\\n    # Initialize GradientBoostingClassifier\\n    gb_clf = GradientBoostingClassifier()\\n    algorithms.append({\\'algo\\': gb_clf, \\'scale\\': True})\\n\\n    # Creating a Gaussian Naive Bayes model\\n    gnb = GaussianNB()\\n    algorithms.append({\\'algo\\': gnb, \\'scale\\': False})\\n\\n    results = []\\n\\n    # Loop over the algorithms list and apply each one\\n    for algorithm in algorithms:\\n        current_x = X_train\\n        current_x_test = X_test\\n\\n        if algorithm[\\'scale\\']:\\n            current_x = scaled_X_train\\n            current_x_test = scaled_X_test\\n\\n        algorithm[\\'algo\\'].fit(current_x, y_train)\\n        y_pred = algorithm[\\'algo\\'].predict(current_x_test)\\n        accuracy = accuracy_score(y_test, y_pred)\\n        results.append({\\'classifier\\': type(algorithm[\\'algo\\']).__name__, \\'accuracy\\': accuracy})\\n\\n    return results\\n\\n```\\n\\nAnd to test multiple datasets let\\'s create a file `test_multiple_datasets.py`\\n\\n```\\nfrom sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\\nfrom classifiers_set import run_classifiers\\n\\ndatasets = [load_iris, load_digits, load_wine, load_breast_cancer]\\n\\nfor dataset in datasets:\\n    X, y = dataset(return_X_y=True)\\n    results = run_classifiers(X, y)\\n    print(f\"Results for {dataset.__name__}:\")\\n    for result in results:\\n        print(f\"Classifier: {result[\\'classifier\\']}, Accuracy: {result[\\'accuracy\\']}\")\\n\\n\\n```\\n\\nTo check the results: `python test_multiple_datasets.py`'}]}],\n",
       "   'code_review': {'feedback_text': {}, 'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': \"The assistant's response contains a redundant word which could lead to confusion. The phrase 'I'm wondering about about a more difficult dataset' should be corrected to 'I'm wondering about a more difficult dataset' to improve clarity.\",\n",
       "     'minor_issues': '',\n",
       "     'summary': 'The response has a minor repetition error that affects clarity.'},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': \"Huh. These basic classifiers tore through all the datasets you've tried. I think what would be interesting to do now is to get back to our iris dataset and see how the classifiers handle noise in the dataset. Add 5% disturbance for each feature.\"}]},\n",
       "    {'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Interesting result and a great idea on how to explore these classifiers more!\\nTo add noise to the training set of a dataset and run through the classifiers we have you will need to modify the run classifiers function:\\n\\nReplace\\n```\\ndef run_classifiers(X, y):\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n```\\nwith\\n```\\ndef run_classifiers(X, y, X_test=None, y_test=None):\\n    if X_test is None:\\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    else:\\n        X_train, y_train = X, y\\n```\\nWith this you will be able to provide run_classifiers with the modified training data and unmodified test data.\\n\\nNow create a file `test_with_noise.py`\\n\\n```\\nfrom sklearn.datasets import load_iris\\nfrom classifiers_set import run_classifiers\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\n\\ndef add_noise(X_train, noise_level=0.05):\\n    \"\"\"\\n    Function to add uniform random noise to the features in %\\n    \"\"\"\\n    noise = np.random.uniform(-noise_level, noise_level, X_train.shape)\\n    return X_train + X_train*noise\\n\\ndef run_experiment(noise_level):\\n    \"\"\"\\n    Function to run the experiment with the classifiers.\\n    \"\"\"\\n    X, y = load_iris(return_X_y=True)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    X_train_noisy = add_noise(X_train, noise_level)\\n    results = run_classifiers(X_train_noisy, y_train, X_test, y_test)\\n    print(f\"Results for {load_iris.__name__}:\")\\n    for result in results:\\n        print(f\"Classifier: {result[\\'classifier\\']}, Accuracy: {result[\\'accuracy\\']}\")\\n\\nif __name__ == \"__main__\":\\n    run_experiment(0.05)\\n```\\n\\nRun it with `python test_with_noise.py`'}]}],\n",
       "   'code_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': '',\n",
       "     'summary': 'There is no code to review in the LLM_REPLY, as it only contains a suggestion for further experimentation.'},\n",
       "    'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The phrase 'Huh.' at the beginning of the response is informal and could be perceived as unprofessional or dismissive in certain contexts. It would be more appropriate to start with a phrase that acknowledges the user's previous input in a neutral or positive manner.\",\n",
       "     'summary': 'The response is clear and relevant, but the informal opening could be improved for professionalism.'},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Result is still too good. Do a grid search over the noise level to find a tipping point.'}]},\n",
       "    {'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Wonderful idea. Here\\'s the code to look for a noise level that degrades at least one classifier to less than 50% accuracy.\\nI\\'ve split the noise level into 5% increments for this search.\\n\\nAdd ```return results``` to `test_with_noise.py` file at the end of run_experiment function.\\n\\nThen create a file `noise_level_search_on_iris.py`\\n\\n```\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom test_with_noise import run_experiment\\nimport numpy as np\\n\\n\\ndef grid_search_noise_levels():\\n    \"\"\"\\n    Function to perform a grid search over the noise levels.\\n    \"\"\"\\n    noise_levels = np.linspace(0, 1, 21)\\n    for noise_level in noise_levels:\\n        print(\\'-\\'*60)\\n        print(f\"Results for noise level {noise_level}:\")\\n        results = run_experiment(noise_level)\\n        if any(result[\\'accuracy\\'] < 0.5 for result in results):\\n            print(\\'First <0.5 accuracy result\\')\\n            return\\n\\ngrid_search_noise_levels()\\n\\n```\\n\\nRun it with: `python noise_level_search_on_iris.py`'}]}],\n",
       "   'code_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': '',\n",
       "     'summary': 'There is no code to review in the LLM_REPLY, as it only contains a suggestion for further action.'},\n",
       "    'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': 'The response could be more informative by briefly explaining the rationale behind performing a grid search over the noise level, which would provide context and enhance understanding for the user.',\n",
       "     'summary': \"The assistant's reply is clear and concise, but it could benefit from a brief explanation to add context.\"},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'This is too much information on the screen for me. I want you to show me how to plot this data as a line for each of the classifiers depending on the noise level.'}]},\n",
       "    {'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'To plot the results, you will need to install another package - matplotlib.\\nRun `pip install matplotlib`.\\n\\nNext, replace the `noise_level_search_on_iris.py` with the following code:\\n\\n```\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom test_with_noise import run_experiment\\nimport numpy as np\\n\\n\\ndef grid_search_noise_levels():\\n    \"\"\"\\n    Function to perform a grid search over the noise levels.\\n    \"\"\"\\n    noise_levels = np.linspace(0, 1, 21)\\n    all_results = []\\n    for noise_level in noise_levels:\\n        print(\\'-\\'*60)\\n        print(f\"Results for noise level {noise_level}:\")\\n        results = run_experiment(noise_level)\\n        all_results.append((noise_level, results))\\n        if any(result[\\'accuracy\\'] < 0.5 for result in results):\\n            return all_results\\n    return all_results\\n\\ndef plot_results(all_results):\\n    \"\"\"\\n    Function to plot the results.\\n    \"\"\"\\n    classifier_names = [result[\\'classifier\\'] for result in all_results[0][1]]\\n    noise_levels = [result[0] for result in all_results]\\n    for i, classifier_name in enumerate(classifier_names):\\n        accuracies = [result[1][i][\\'accuracy\\'] for result in all_results]\\n        plt.plot(noise_levels, accuracies, label=classifier_name)\\n    plt.xlabel(\\'Noise level\\')\\n    plt.ylabel(\\'Accuracy\\')\\n    plt.legend()\\n    plt.savefig(\\'results.png\\')\\n\\nall_results = grid_search_noise_levels()\\nplot_results(all_results)\\n```\\n\\nAfter running `python noise_level_search_on_iris.py` you should see `results.png` file in the folder with the chart we created.'}]}],\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The response could be more concise by directly addressing the user's request for a visualization method without mentioning the perceived information overload.\",\n",
       "     'summary': \"The assistant's reply is clear and free of mistakes but could be more concise.\"},\n",
       "    'score': 4},\n",
       "   'code_review': {'feedback_text': {'critical_issues': 'The LLM did not provide any code in its response, which was necessary for the task at hand.',\n",
       "     'summary': \"The absence of code in the LLM's response is a critical issue as it was required to fulfill the user's request.\"},\n",
       "    'score': 1}},\n",
       "  {'turn': [{'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': \"I've generated the image. Give me a cluster bar chart for this thing.\\nBut reduce number of noise points to one per 30%. Make the bars thin as we have lots of classifiers.\"}]},\n",
       "    {'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Sure thing. Add this function to the `noise_level_search_on_iris.py` after `plot_results(all_results)`\\n\\n```\\ndef plot_results_cluster_bar(all_results):\\n    \"\"\"\\n    Function to plot the results.\\n    \"\"\"\\n    classifier_names = [result[\\'classifier\\'] for result in all_results[0][1]]\\n    print(all_results)\\n    noise_levels = [result[0] for result in all_results if result[0] % 0.3 < 0.001]\\n    print(noise_levels)\\n    width = 0.1 / len(classifier_names)  # Adjust bar width based on number of classifiers\\n    fig, ax = plt.subplots()\\n    for i, classifier_name in enumerate(classifier_names):\\n        accuracies = [result[1][i][\\'accuracy\\'] for result in all_results if result[0] % 0.3  < 0.001]\\n        ax.bar([noise + i * width for noise in noise_levels], accuracies, width, label=classifier_name)\\n    ax.set_xlabel(\\'Noise level\\')\\n    ax.set_ylabel(\\'Accuracy\\')\\n    ax.legend()\\n    plt.savefig(\\'results_cluster_bar.png\\')\\n```\\n\\nand then add this line at the end of the file to create the new chart - `plot_results_cluster_bar(all_results)`.\\n\\nThe new image will be named `results_cluster_bar.png`.'}]}],\n",
       "   'code_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': '',\n",
       "     'summary': 'There is no code to review in the LLM_REPLY, as it only contains a text response without any code snippet.'},\n",
       "    'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The response could be more concise by eliminating unnecessary words and phrases. For example, 'Give me a cluster bar chart for this thing.' could be rephrased to 'Please generate a cluster bar chart.' to maintain professionalism and clarity.\",\n",
       "     'summary': 'The response is clear and free of major errors but could be slightly more concise and professional.'},\n",
       "    'score': 4}},\n",
       "  {'turn': [{'role': 'llm',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': 'Thanks! What was the point of doing < 0.001 in the code above instead of == 0 though?'}]},\n",
       "    {'role': 'human',\n",
       "     'steps': [{'type': 'markdown',\n",
       "       'content': \"I've used < 0.001 comparisons instead of == 0 to avoid problems with the float point numbers precision. As we used `np.linspace(0, 1, 21)` to generate the noise levels, we might not get precise values for 0.3 0.6 0.9 even though we've separated [0, 1] into 21 points due to how float numbers are stored in the computer. So to avoid a situation, where a very small difference prevents numbers from being seen as equal, I've added a threshold to ignore these small differences.\"}]}],\n",
       "   'code_review': {'feedback_text': {'summary': \"There is no code to review in the LLM's reply, as it only contains a question for clarification.\"},\n",
       "    'score': 5},\n",
       "   'english_review': {'feedback_text': {'critical_issues': '',\n",
       "     'medium_issues': '',\n",
       "     'minor_issues': \"The Assistant's reply is a question that seems to be missing context. It would be more helpful if the Assistant provided an explanation for the use of '< 0.001' instead of '== 0', as this would directly address the user's likely confusion.\",\n",
       "     'summary': \"The Assistant's response is a question that lacks context and does not provide an explanation, which would be more beneficial for the user.\"},\n",
       "    'score': 4}}],\n",
       " 'nb_path': 'src/llm_reviewer/tests/samples/10T__ml_research.ipynb'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.turn_reviewer import load_rubric, load_system_prompt\n",
    "\n",
    "\n",
    "CODE_PROMPT = load_system_prompt('aggregated_reviews_code_judge')\n",
    "CODE_RUBRICS = load_rubric('code_reviewer')\n",
    "LANG_PROMPT = load_system_prompt('aggregated_reviews_english_judge')\n",
    "LANG_RUBRICS = load_rubric('english_reviewer')\n",
    "\n",
    "\n",
    "from src.llm_reviewer.llm_api import make_llm_request, LLMAPIFactory\n",
    "\n",
    "def get_judgment(reviewer, feedback):\n",
    "    llm_client = LLMAPIFactory().get()\n",
    "    if reviewer == 'code':\n",
    "        prompt = CODE_PROMPT\n",
    "        rubrics = CODE_RUBRICS\n",
    "    elif reviewer == 'lang':\n",
    "        prompt = LANG_PROMPT\n",
    "        rubrics = LANG_RUBRICS\n",
    "    else:\n",
    "        raise ValueError(\"Reviewer type not recognized.\")\n",
    "    \n",
    "    formatted_prompt = prompt.format(FEEDBACK=feedback, GRADING_RUBRICS=rubrics)\n",
    "    judgment = make_llm_request(\n",
    "        llm_client,\n",
    "        [{'role': 'system', 'content': formatted_prompt}],\n",
    "        'gpt-4-1106-preview',\n",
    "        temperature= 0.0,\n",
    "        max_tokens = 4000,\n",
    "        response_format = {'type': \"json_object\"},\n",
    "        retries = 3,\n",
    "    )\n",
    "    return judgment\n",
    "\n",
    "# Example of running the function\n",
    "#judgment = get_judgment(CODE_PROMPT, gpt_reviews_df.iloc[0]['code_feedback'])\n",
    "#judgment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import notebook_reviews_to_df, IssueLevel\n",
    "\n",
    "df = notebook_reviews_to_df(filter(None, [review]), IssueLevel.MEDIUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_path</th>\n",
       "      <th>code_score</th>\n",
       "      <th>lang_score</th>\n",
       "      <th>comb_feedback</th>\n",
       "      <th>code_feedback</th>\n",
       "      <th>lang_feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>src/llm_reviewer/tests/samples/10T__ml_researc...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>#Turn 1:\\n\\n## Language(4/5):\\n**Medium_Issues...</td>\n",
       "      <td>#Turn 1:\\n\\n## Code(4/5):\\n**Medium_Issues**\\n...</td>\n",
       "      <td>#Turn 1:\\n\\n## Language(4/5):\\n**Medium_Issues...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             nb_path  code_score  lang_score  \\\n",
       "0  src/llm_reviewer/tests/samples/10T__ml_researc...         3.8         4.0   \n",
       "\n",
       "                                       comb_feedback  \\\n",
       "0  #Turn 1:\\n\\n## Language(4/5):\\n**Medium_Issues...   \n",
       "\n",
       "                                       code_feedback  \\\n",
       "0  #Turn 1:\\n\\n## Code(4/5):\\n**Medium_Issues**\\n...   \n",
       "\n",
       "                                       lang_feedback  \n",
       "0  #Turn 1:\\n\\n## Language(4/5):\\n**Medium_Issues...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'judgment': 'The code provided across turns generally functions correctly but often lacks error handling, efficiency improvements, and necessary function definitions, indicating room for significant enhancement.',\n",
       " 'score': '3'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_judgment('code', df.iloc[0]['code_feedback'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
