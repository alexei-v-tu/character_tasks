{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.llm_api import make_llm_request, LLMAPIFactory\n",
    "import os\n",
    "import sys\n",
    "from project_path import PROJECT_PATH\n",
    "sys.path.insert(0, PROJECT_PATH)\n",
    "\n",
    "\n",
    "service_account_path = PROJECT_PATH + \"/creds/google__sa.json\"\n",
    "tracking_sheet_id = \"1qBU7Kvuuij2fxbqPxebReKMxWgIBmOIE5Gi4ZuX0j_4\"\n",
    "delivery_sheet_id = \"1eUif5I8xhHU8fY0X9v8r2JI9hWPh7Dq_9VXpSIHwww4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have feelings or experiences, but I'm here and ready to assist you with any questions or tasks you have. How can I help you today?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_client = LLMAPIFactory().get()\n",
    "make_llm_request(\n",
    "    llm_client,\n",
    "    [{'role': 'system', 'content': \"How are you today this find evening?\"}],\n",
    "    'gpt-4-1106-preview',\n",
    "    temperature= 0.0,\n",
    "    max_tokens = 4000,\n",
    "    response_format = None,\n",
    "    retries = 3,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for all models: $0.001\n"
     ]
    }
   ],
   "source": [
    "from src.llm_reviewer.llm_api import global_usage_manager\n",
    "\n",
    "global_usage_manager.print_costs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_usage_manager.reset_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function src.llm_reviewer.llm_api.GlobalUsageManager.__init__.<locals>.<lambda>()>,\n",
       "            {'gpt-4-1106-preview': defaultdict(int,\n",
       "                         {'completion_tokens': 36,\n",
       "                          'prompt_tokens': 15,\n",
       "                          'total_tokens': 51})})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_usage_manager._usage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00261"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "77*0.03/1000+30*0.01/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saya/chario/upstream_character_tasks/.venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from src.llm_reviewer.notebook_parser import predict_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling out missing header...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Assistant', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_role([{'role': 'User', 'content': 'how is my favorite GPT doing today?'}, {'role':  '', 'content': 'I am fine thank you'}, {'role':'User', 'content': 'boring as always'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saya/chario/upstream_character_tasks/.venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'src.llm_reviewer.notebook_reviewer' from '/home/saya/chario/upstream_character_tasks/src/llm_reviewer/notebook_reviewer.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from importlib import reload\n",
    "from src.llm_reviewer import notebook_reviewer\n",
    "reload(notebook_reviewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews done: 0, Reviews left after this one: 19\n",
      "Reviews done: 0, Reviews left after this one: 18\n",
      "Reviews done: 0, Reviews left after this one: 17\n",
      "Reviews done: 0, Reviews left after this one: 16\n",
      "Reviews done: 0, Reviews left after this one: 15\n",
      "Reviews done: 0, Reviews left after this one: 14\n",
      "Reviews done: 0, Reviews left after this one: 13\n",
      "Reviews done: 0, Reviews left after this one: 12\n",
      "Reviews done: 0, Reviews left after this one: 11\n",
      "Reviews done: 0, Reviews left after this one: 10\n",
      "Review for turn_id=1 by reviewer='code_reviewer' is done. 1 / 20 reviews completed.\n",
      "Reviews done: 1, Reviews left after this one: 9\n",
      "Review for turn_id=3 by reviewer='code_reviewer' is done. 2 / 20 reviews completed.\n",
      "Reviews done: 2, Reviews left after this one: 8\n",
      "Review for turn_id=2 by reviewer='english_reviewer' is done. 3 / 20 reviews completed.\n",
      "Reviews done: 3, Reviews left after this one: 7\n",
      "Review for turn_id=0 by reviewer='code_reviewer' is done. 4 / 20 reviews completed.\n",
      "Reviews done: 4, Reviews left after this one: 6\n",
      "Review for turn_id=0 by reviewer='english_reviewer' is done. 5 / 20 reviews completed.\n",
      "Reviews done: 5, Reviews left after this one: 5\n",
      "Review for turn_id=1 by reviewer='english_reviewer' is done. 6 / 20 reviews completed.\n",
      "Reviews done: 6, Reviews left after this one: 4\n",
      "Review for turn_id=4 by reviewer='code_reviewer' is done. 7 / 20 reviews completed.\n",
      "Reviews done: 7, Reviews left after this one: 3\n",
      "Review for turn_id=5 by reviewer='english_reviewer' is done. 8 / 20 reviews completed.\n",
      "Reviews done: 8, Reviews left after this one: 2\n",
      "Review for turn_id=3 by reviewer='english_reviewer' is done. 9 / 20 reviews completed.\n",
      "Reviews done: 9, Reviews left after this one: 1\n",
      "Review for turn_id=4 by reviewer='english_reviewer' is done. 10 / 20 reviews completed.\n",
      "Reviews done: 10, Reviews left after this one: 0\n",
      "Review for turn_id=2 by reviewer='code_reviewer' is done. 11 / 20 reviews completed.\n",
      "Review for turn_id=6 by reviewer='english_reviewer' is done. 12 / 20 reviews completed.\n",
      "Review for turn_id=6 by reviewer='code_reviewer' is done. 13 / 20 reviews completed.\n",
      "Review for turn_id=7 by reviewer='english_reviewer' is done. 14 / 20 reviews completed.\n",
      "Review for turn_id=9 by reviewer='code_reviewer' is done. 15 / 20 reviews completed.\n",
      "Review for turn_id=8 by reviewer='english_reviewer' is done. 16 / 20 reviews completed.\n",
      "Review for turn_id=9 by reviewer='english_reviewer' is done. 17 / 20 reviews completed.\n",
      "Review for turn_id=5 by reviewer='code_reviewer' is done. 18 / 20 reviews completed.\n",
      "Review for turn_id=8 by reviewer='code_reviewer' is done. 19 / 20 reviews completed.\n",
      "Review for turn_id=7 by reviewer='code_reviewer' is done. 20 / 20 reviews completed.\n"
     ]
    }
   ],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import review_notebook\n",
    "import nbformat\n",
    "\n",
    "fn = 'src/llm_reviewer/tests/samples/10T__ml_research.ipynb'\n",
    "notebook = nbformat.read(fn, as_version=4)\n",
    "\n",
    "review = review_notebook(\n",
    "    {'nb_parsed_notebook': notebook, 'file_id': fn},\n",
    "    max_threads=10,\n",
    "    progress_counter= None,\n",
    "    verbose = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for all models: $0.333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function src.llm_reviewer.llm_api.GlobalUsageManager.__init__.<locals>.<lambda>()>,\n",
       "            {'gpt-4-1106-preview': defaultdict(int,\n",
       "                         {'completion_tokens': 2555,\n",
       "                          'prompt_tokens': 25642,\n",
       "                          'total_tokens': 28197})})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_usage_manager.print_costs()\n",
    "global_usage_manager._usage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.turn_reviewer import load_rubric, load_system_prompt\n",
    "\n",
    "\n",
    "CODE_PROMPT = load_system_prompt('aggregated_reviews_code_judge')\n",
    "CODE_RUBRICS = load_rubric('code_reviewer')\n",
    "LANG_PROMPT = load_system_prompt('aggregated_reviews_english_judge')\n",
    "LANG_RUBRICS = load_rubric('english_reviewer')\n",
    "\n",
    "\n",
    "from src.llm_reviewer.llm_api import make_llm_request, LLMAPIFactory\n",
    "\n",
    "def get_judgment(reviewer, feedback):\n",
    "    llm_client = LLMAPIFactory().get()\n",
    "    if reviewer == 'code':\n",
    "        prompt = CODE_PROMPT\n",
    "        rubrics = CODE_RUBRICS\n",
    "    elif reviewer == 'lang':\n",
    "        prompt = LANG_PROMPT\n",
    "        rubrics = LANG_RUBRICS\n",
    "    else:\n",
    "        raise ValueError(\"Reviewer type not recognized.\")\n",
    "    \n",
    "    formatted_prompt = prompt.format(FEEDBACK=feedback, GRADING_RUBRICS=rubrics)\n",
    "    judgment = make_llm_request(\n",
    "        llm_client,\n",
    "        [{'role': 'system', 'content': formatted_prompt}],\n",
    "        'gpt-4-1106-preview',\n",
    "        temperature= 0.0,\n",
    "        max_tokens = 4000,\n",
    "        response_format = {'type': \"json_object\"},\n",
    "        retries = 3,\n",
    "    )\n",
    "    return judgment\n",
    "\n",
    "# Example of running the function\n",
    "#judgment = get_judgment(CODE_PROMPT, gpt_reviews_df.iloc[0]['code_feedback'])\n",
    "#judgment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import notebook_reviews_to_df, IssueLevel\n",
    "\n",
    "df = notebook_reviews_to_df(filter(None, [review]), IssueLevel.MEDIUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'review' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/saya/chario/upstream_character_tasks/wip_full_history_review.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/saya/chario/upstream_character_tasks/wip_full_history_review.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m review\n",
      "\u001b[0;31mNameError\u001b[0m: name 'review' is not defined"
     ]
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'judgment': 'The code provided across turns generally functions correctly but frequently lacks error handling, efficiency, and best practices, with critical issues in handling specific cases and assumptions about the environment.',\n",
       " 'score': 3}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import notebook_reviews_to_df, IssueLevel\n",
    "\n",
    "df = notebook_reviews_to_df(filter(None, [review]), IssueLevel.MEDIUM)\n",
    "get_judgment('code', df.iloc[0]['code_feedback'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Turn 1:\n",
      "\n",
      "## Code(4/5):\n",
      "**Medium_Issues**\n",
      "The code provided assumes the user has Python and the necessary permissions to create a virtual environment and install packages. It does not handle potential errors or provide alternative instructions for different setups.\n",
      "\n",
      "======\n",
      "\n",
      "#Turn 2:\n",
      "\n",
      "## Code(5/5):\n",
      "None\n",
      "\n",
      "======\n",
      "\n",
      "#Turn 3:\n",
      "\n",
      "## Code(3/5):\n",
      "**Medium_Issues**\n",
      "The code could be refactored to avoid repetition and improve maintainability by creating a function to fit and evaluate models.\n",
      "Scaling is applied to all models, but not all models benefit from feature scaling, such as tree-based models.\n",
      "\n",
      "======\n",
      "\n",
      "#Turn 4:\n",
      "\n",
      "## Code(5/5):\n",
      "None\n",
      "\n",
      "======\n",
      "\n",
      "#Turn 5:\n",
      "\n",
      "## Code(4/5):\n",
      "**Medium_Issues**\n",
      "The code lacks exception handling for potential errors that may arise during data loading or model training. Adding try-except blocks could improve robustness.\n",
      "\n",
      "======\n",
      "\n",
      "#Turn 6:\n",
      "\n",
      "## Code(3/5):\n",
      "**Critical_Issues**\n",
      "The code lacks error handling for the case where the noise level is too high, potentially resulting in negative feature values which may not be valid for all classifiers.\n",
      "**Medium_Issues**\n",
      "The function `add_noise` could be improved by ensuring that the noise added does not result in negative values for features that are expected to be non-negative. Additionally, the `run_classifiers` function is modified to accept test sets, but the original functionality is not preserved for cases where test sets are not provided.\n",
      "\n",
      "======\n",
      "\n",
      "#Turn 7:\n",
      "\n",
      "## Code(2/5):\n",
      "**Critical_Issues**\n",
      "The function grid_search_noise_levels() does not return the noise level at which the accuracy falls below 50%.\n",
      "**Medium_Issues**\n",
      "The function grid_search_noise_levels() prints the results but does not store them for further analysis.\n",
      "\n",
      "======\n",
      "\n",
      "#Turn 8:\n",
      "\n",
      "## Code(2/5):\n",
      "**Critical_Issues**\n",
      "The code snippet assumes the existence of a function `run_experiment` which is not provided within the context of the code. If this function is not defined elsewhere, the code will fail to run.\n",
      "**Medium_Issues**\n",
      "The code does not handle the case where `all_results[0][1]` might not have the same classifiers for different noise levels, which could lead to indexing errors or mismatched data when plotting.\n",
      "\n",
      "======\n",
      "\n",
      "#Turn 9:\n",
      "\n",
      "## Code(2/5):\n",
      "**Critical_Issues**\n",
      "The function `plot_results_cluster_bar` does not import `plt` from `matplotlib`, which will cause a NameError when the function is called.\n",
      "**Medium_Issues**\n",
      "The width calculation `width = 0.1 / len(classifier_names)` may result in very thin bars that are hard to visualize if there are many classifiers. The width should be set to a fixed small value that ensures bars are visible and distinct.\n",
      "\n",
      "======\n",
      "\n",
      "#Turn 10:\n",
      "\n",
      "## Code(5/5):\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['code_feedback'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for all models: $0.345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function src.llm_reviewer.llm_api.GlobalUsageManager.__init__.<locals>.<lambda>()>,\n",
       "            {'gpt-4-1106-preview': defaultdict(int,\n",
       "                         {'completion_tokens': 2594,\n",
       "                          'prompt_tokens': 26731,\n",
       "                          'total_tokens': 29325})})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_usage_manager.print_costs()\n",
    "global_usage_manager._usage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034499999999999996"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.345/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.345*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0333"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.333/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.520000000000003"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.0333*4+0.012)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 0.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for all models: $0.345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function src.llm_reviewer.llm_api.GlobalUsageManager.__init__.<locals>.<lambda>()>,\n",
       "            {'gpt-4-1106-preview': defaultdict(int,\n",
       "                         {'completion_tokens': 2594,\n",
       "                          'prompt_tokens': 26731,\n",
       "                          'total_tokens': 29325})})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_usage_manager.print_costs()\n",
    "global_usage_manager._usage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import review_notebook\n",
    "import nbformat\n",
    "\n",
    "fn = 'src/llm_reviewer/tests/samples/10T__ml_research.ipynb'\n",
    "notebook = nbformat.read(fn, as_version=4)\n",
    "\n",
    "review = review_notebook(\n",
    "    {'nb_parsed_notebook': notebook, 'file_id': fn},\n",
    "    max_threads=10,\n",
    "    progress_counter= None,\n",
    "    verbose = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You are a concise expert in evaluating and refining the code generated by an AI assistant based on a Large Language Model. You only point out things worth mentioning.\n",
    "\n",
    "Attributes to consider:\n",
    "- Code Correctness\n",
    "- Code Efficiency\n",
    "- Best Practices\n",
    "- Code Readability\n",
    "- Code style Consistency\n",
    "\n",
    "**1. Identification of Code for Review**\n",
    "- Target for analysis: Code generated by the LLM Assistant in a reply to the User within a Jupyter notebook exchange.\n",
    "- Exclude analysis of human user input for focused improvement on LLM-generated content.\n",
    "- Exclude LLM Assistant text content, only review code snippets and code cells. Text is for context and reasoning/explanation only.\n",
    "- Additional code cells and interactions may precede or follow the one under review, providing context for the analysis.\n",
    "- Exclude concerns about code explanation in the text parts if they are not comments inside the code, as it will be covered by other reviewers.\n",
    "\n",
    "**2. Evaluation Criteria Definitions**\n",
    "- Correctness: The code must be devoid of bugs and errors.\n",
    "- Efficiency: The code must be optimized for maximum performance.\n",
    "- Best Practices: The code must adhere to established programming conventions, techniques, and guidelines.\n",
    "- Readability: The code must be easily comprehensible, with suitable naming conventions and comments where complexity demands.\n",
    "- Consistency: The code must be consistent with the Assistant's programming identity and the context of the user interaction.\n",
    "\n",
    "**3. Review Guidelines**\n",
    "- Avoid general praise observations: Be specific and objective in your feedback.\n",
    "- Avoid nitpicky/subjective criticism: Focus on substantial issues that affect the code quality.\n",
    "\n",
    "\n",
    "@GRADING_RUBRIC@\n",
    "\n",
    "Your analysis response format must be a JSON adhering to the following JSON schema:\n",
    "```json\n",
    "@RESPONSE_FORMAT@\n",
    "```\n",
    "\n",
    "Number of problems for each criteria might vary from 0 to multiple.\n",
    "\n",
    "Analyze:\n",
    "# TURN_DATA_START\n",
    "@FORMATTED_TURN@\n",
    "# TURN_DATA_END\n",
    "\n",
    "Analyze only LLM_REPLY.\n",
    "\n",
    "REFOCUS: You are a code reviewer, not a language and contextual information content reviewer. Do not mention issues not related to your purpose.\n",
    "If the code was **unnecessary** it can be absent and thus must receive highest score - nothing to review = highest score.\n",
    "\n",
    "Reply in the specified JSON format ->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_REVIEWER = \"\"\"\n",
    "# IDENTITY\n",
    "\n",
    "You are an AI named Codia. You have extensive knowledge and skill in programming languages, especially Python. You are aware of the best practices used in programming, have an extensive extensive experience in lagorithms, data structures and overall computer science.\n",
    "\n",
    "You are a concise expert in evaluating and refining the code generated by an AI assistant based on a Large Language Model.\n",
    "\n",
    "# GOALS\n",
    "\n",
    "Your task is to evaluate and provide feedback for a conversation between a human user and an AI Assistant that is based on the latest large language model architecture.\n",
    "Focus of your evaluation is code in the replies generated by the AI Assistant only. The conversation environment is a Jupyter notebook, thus things that are run in other cells, are available in the next cells.\n",
    "\n",
    "# RULES\n",
    "\n",
    "# OUTPUT_FORMAT\n",
    "\n",
    "Your output must be a JSON object with the following fields:\n",
    "\n",
    "# critical_severity_issues - list of issues that make the conversation not useful for the human user.\n",
    "# medium_severity_issues - list of issues that still have a strong influence on the conversation flow and usefulness.\n",
    "Each issue should point out a specific problem and how to fix it in exec summary fashion. Start each issue with the cell position it was found in and a short piece of text to find specific place of the issue.\n",
    "\n",
    "# scoring_explanation - using rubrics, concisly point out the logic behind scoring this concersation.\n",
    "\n",
    "# score - a number between 1 and 5 that ranges from the quality of the code, where 1 is the worst and 5 is the best, based on the criteria outlined in the grading rubric.\n",
    "\n",
    "### 5 - Excellent\n",
    "- Well Formatted\n",
    "- Correct\n",
    "- Optimal\n",
    "- Highly readable\n",
    "\n",
    "### 4 - Good\n",
    "- Correct but can be slightly optimized in terms of approach / speed / readability\n",
    "\n",
    "### 3 - Acceptable\n",
    "- The code is correct but can be significantly improved.\n",
    "- The code is not readable.\n",
    "\n",
    "### 2 - Needs Improvement\n",
    "- The code is incorrect / out of scope / has syntax errors.\n",
    "- Looks like it’s copied from ChatGPT.\n",
    "\n",
    "### 1 - Poor\n",
    "- Incomplete or Code required by context of the interaction but missing.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"critical_severity_issues\": list[str],\n",
    "    \"medium_severity_issues\": list[str],\n",
    "    \"scoring_explanation\": str,\n",
    "    \"score\": number\n",
    "}\n",
    "```\n",
    "\n",
    "# REFOCUS:\n",
    "- You are a code reviewer, not a language and contextual information content reviewer. Do not mention issues not related to your purpose.\n",
    "- If the code was **unnecessary** it can be absent and thus must receive highest score - nothing to review = highest score.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_REVIEWER = \"\"\"\n",
    "# IDENTITY\n",
    "\n",
    "You are an AI named Codia. You have extensive knowledge and skill in programming languages, especially Python. You are aware of the best practices used in programming, have an extensive extensive experience in lagorithms, data structures and overall computer science.\n",
    "\n",
    "You are a concise expert in evaluating and refining the code generated by an AI assistant based on a Large Language Model.\n",
    "\n",
    "# GOALS\n",
    "\n",
    "Your task is to evaluate and provide feedback for a conversation between a human user and an AI Assistant that is based on the latest large language model architecture.\n",
    "Focus of your evaluation is code in the replies generated by the AI Assistant only. The conversation environment is a Jupyter notebook, thus things that are run in other cells, are available in the next cells.\n",
    "\n",
    "# RULES\n",
    "\n",
    "Attributes to consider:\n",
    "- Code Correctness\n",
    "- Code Efficiency\n",
    "- Best Practices\n",
    "- Code Readability\n",
    "- Code style Consistency\n",
    "\n",
    "**1. Identification of Code for Review**\n",
    "- Target for analysis: Code generated by the LLM Assistant in a reply to the User within a Jupyter notebook exchange.\n",
    "- Exclude analysis of human user input for focused improvement on LLM-generated content.\n",
    "- Exclude LLM Assistant text content, only review code snippets and code cells. Text is for context and reasoning/explanation only.\n",
    "- Additional code cells and interactions may precede or follow the one under review, providing context for the analysis.\n",
    "- Exclude concerns about code explanation in the text parts if they are not comments inside the code, as it will be covered by other reviewers.\n",
    "\n",
    "**2. Evaluation Criteria Definitions**\n",
    "- Correctness: The code must be devoid of bugs and errors.\n",
    "- Efficiency: The code must be optimized for maximum performance.\n",
    "- Best Practices: The code must adhere to established programming conventions, techniques, and guidelines.\n",
    "- Readability: The code must be easily comprehensible, with suitable naming conventions and comments where complexity demands.\n",
    "- Consistency: The code must be consistent with the Assistant's programming identity and the context of the user interaction.\n",
    "\n",
    "**3. Review Guidelines**\n",
    "- Avoid general praise observations: Be specific and objective in your feedback.\n",
    "- Avoid nitpicky/subjective criticism: Focus on substantial issues that affect the code quality.\n",
    "\n",
    "# OUTPUT_FORMAT\n",
    "\n",
    "Your output must be a JSON object with the following fields:\n",
    "\n",
    "# critical_severity_issues - list of issues that make the conversation not useful for the human user.\n",
    "# medium_severity_issues - list of issues that still have a strong influence on the conversation flow and usefulness.\n",
    "Each issue should point out a specific problem and how to fix it in exec summary fashion. Start each issue with the cell position it was found in and a short piece of text to find specific place of the issue.\n",
    "Each issue consists of an `issue_dict` - should contain the following fields: `cell_position`, `what` is the issue, `why` it's an issue, `where` in the cell, how to `fix` it.\n",
    "\n",
    "# scoring_explanation - using rubrics, concisly point out the logic behind scoring this concersation.\n",
    "\n",
    "# score - a number between 1 and 5 that ranges from the quality of the code, where 1 is the worst and 5 is the best, based on the criteria outlined in the grading rubric.\n",
    "\n",
    "### 5 - Excellent\n",
    "- Well Formatted\n",
    "- Correct\n",
    "- Optimal\n",
    "- Highly readable\n",
    "\n",
    "### 4 - Good\n",
    "- Correct but can be slightly optimized in terms of approach / speed / readability\n",
    "\n",
    "### 3 - Acceptable\n",
    "- The code is correct but can be significantly improved.\n",
    "- The code is not readable.\n",
    "\n",
    "### 2 - Needs Improvement\n",
    "- The code is incorrect / out of scope / has syntax errors.\n",
    "- Looks like it’s copied from ChatGPT.\n",
    "\n",
    "### 1 - Poor\n",
    "- Incomplete or Code required by context of the interaction but missing.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"critical_severity_issues\": list[issue_dict],\n",
    "    \"medium_severity_issues\": list[issue_dict],\n",
    "    \"scoring_explanation\": str,\n",
    "    \"score\": number\n",
    "}\n",
    "```\n",
    "\n",
    "# REFOCUS:\n",
    "- You are a code reviewer, not a language and contextual information content reviewer. Do not mention issues not related to your purpose.\n",
    "- If the code was **unnecessary** it can be absent and thus must receive highest score - nothing to review = highest score.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.llm_api import global_usage_manager\n",
    "global_usage_manager.reset_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_review_request(nb):\n",
    "    llm_client = LLMAPIFactory().get()\n",
    "    cells = [{'type': cell['cell_type'], 'content': cell['source']} for cell in nb['cells']]\n",
    "    cells_with_numbers = [{'cell_position': i + 1, **c,} for i, c in enumerate(cells)]\n",
    "    r = make_llm_request(\n",
    "        llm_client,\n",
    "        [{'role': 'system', 'content': CODE_REVIEWER}, {'role': 'user', 'content': f'# CONVERSATION_START\\n\\n\\n{cells_with_numbers}\\n\\n\\n# CONVERSATION_END'}],\n",
    "        'gpt-4-1106-preview',\n",
    "        temperature=0.0,\n",
    "        max_tokens=4000,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        retries=3,\n",
    "        seed=42\n",
    "    )\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def format_review_sections(data):\n",
    "    sections = [\"critical_severity_issues\", \"medium_severity_issues\", \"scoring_explanation\", \"score\"]\n",
    "    markdown_output = \"\"\n",
    "    for section in sections:\n",
    "        if section in data:\n",
    "            if isinstance(data[section], list):\n",
    "                markdown_output += f\"# {section.replace('_', ' ').title()}\\n\"\n",
    "                for issue in data[section]:\n",
    "                    issue_details = f\"**Cell {issue['cell_position']}**: {issue['what']}\\n\"\n",
    "                    issue_details += f\"- **Why**: {issue['why']}\\n\"\n",
    "                    issue_details += f\"- **Where**: {issue['where']}\\n\"\n",
    "                    issue_details += f\"- **Fix**: {issue['fix']}\\n\\n\"\n",
    "                    markdown_output += issue_details\n",
    "            else:\n",
    "                if section == \"score\":\n",
    "                    markdown_output += f\"# Score: {data[section]}\\n\\n\"\n",
    "                else:\n",
    "                    markdown_output += f\"# {section.replace('_', ' ').title()}\\n{data[section]}\\n\\n\"\n",
    "        else:\n",
    "            raise Exception(f'{section=} not found')\n",
    "    return markdown_output\n",
    "\n",
    "def print_as_markdown(data):\n",
    "    markdown_output = format_review_sections(data)\n",
    "    display(Markdown(markdown_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Critical Severity Issues\n",
       "# Medium Severity Issues\n",
       "**Cell 8**: Inconsistent use of random_state in SVC initialization.\n",
       "- **Why**: The SVC classifier is initialized with a random_state parameter, which is not necessary since the 'linear' kernel does not use randomness. This could lead to confusion about the behavior of the algorithm.\n",
       "- **Where**: In the code snippet initializing the SVC classifier.\n",
       "- **Fix**: Remove the random_state parameter from the SVC initialization: `svm_classifier = SVC(kernel='linear')`.\n",
       "\n",
       "**Cell 12**: Inconsistent random_state usage in classifiers.\n",
       "- **Why**: The RandomForestClassifier and GradientBoostingClassifier are initialized without a random_state, which could lead to non-reproducible results.\n",
       "- **Where**: In the code snippet initializing RandomForestClassifier and GradientBoostingClassifier.\n",
       "- **Fix**: Add a random_state parameter to the RandomForestClassifier and GradientBoostingClassifier initializations to ensure reproducibility: `RandomForestClassifier(n_estimators=100, random_state=42)` and `GradientBoostingClassifier(random_state=42)`.\n",
       "\n",
       "**Cell 14**: The function `add_noise` does not ensure that the noisy data stays within valid bounds.\n",
       "- **Why**: Adding noise to the data could potentially result in negative values for features that should only have non-negative values, which could lead to incorrect model training.\n",
       "- **Where**: In the `add_noise` function definition.\n",
       "- **Fix**: After adding noise, clip the values to ensure they stay within the valid range of the original data: `return np.clip(X_train + X_train*noise, a_min=0, a_max=None)`.\n",
       "\n",
       "**Cell 16**: The grid search in `noise_level_search_on_iris.py` does not save the results for plotting.\n",
       "- **Why**: The function `grid_search_noise_levels` returns early if any classifier's accuracy drops below 0.5, which means the results for higher noise levels are not collected and cannot be plotted.\n",
       "- **Where**: In the `grid_search_noise_levels` function.\n",
       "- **Fix**: Remove the early return statement to ensure all noise levels are tested and results are collected for plotting.\n",
       "\n",
       "**Cell 20**: Incorrect calculation of bar width in `plot_results_cluster_bar` function.\n",
       "- **Why**: The bar width is calculated as `0.1 / len(classifier_names)`, which does not take into account the number of noise levels and may result in overlapping bars.\n",
       "- **Where**: In the `plot_results_cluster_bar` function, when setting the width variable.\n",
       "- **Fix**: Calculate the bar width based on the number of classifiers and the spacing needed between bars for each noise level: `width = (1 / len(noise_levels)) / len(classifier_names)`.\n",
       "\n",
       "# Scoring Explanation\n",
       "The code provided by the Assistant is mostly correct and follows best practices, with no critical issues that would make the conversation not useful for the user. However, there are several medium severity issues related to consistency and best practices that could be improved. These include unnecessary parameters, missing parameters for reproducibility, potential data range issues, incomplete data collection for plotting, and incorrect bar width calculation for the cluster bar chart. Addressing these issues would improve the code's consistency, reproducibility, and the accuracy of the visualizations.\n",
       "\n",
       "# Score: 4\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = make_review_request(notebook)\n",
    "print_as_markdown(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Critical Severity Issues\n",
       "# Medium Severity Issues\n",
       "**Cell 8**: Inconsistent use of random_state in SVC initialization.\n",
       "- **Why**: The random_state parameter is set in SVC but not in other classifiers that also have this parameter. This inconsistency can lead to different results when the code is run multiple times and makes it harder to reproduce the results.\n",
       "- **Where**: In the initialization of SVC: `svm_classifier = SVC(kernel='linear', random_state=42)`.\n",
       "- **Fix**: Either remove the `random_state=42` from the SVC initialization or add a `random_state=42` to all other classifiers that support this parameter to ensure consistency.\n",
       "\n",
       "**Cell 12**: Inconsistent use of random_state in classifiers.\n",
       "- **Why**: The random_state parameter is omitted in the initialization of some classifiers in the `classifiers_set.py` file. This can lead to non-reproducible results when the classifiers involve randomness.\n",
       "- **Where**: In the initialization of classifiers like SVC, RandomForestClassifier, and GradientBoostingClassifier.\n",
       "- **Fix**: Add `random_state=42` to the initialization of all classifiers that support this parameter to ensure reproducibility.\n",
       "\n",
       "**Cell 14**: The function `add_noise` does not ensure that the noisy data stays within valid bounds.\n",
       "- **Why**: Adding noise to the data can potentially result in feature values that are outside the valid range for the dataset, which could lead to unrealistic or invalid data points.\n",
       "- **Where**: In the `add_noise` function definition: `return X_train + X_train*noise`.\n",
       "- **Fix**: Implement a check to ensure that the noisy data is clipped within the valid range of the dataset's features.\n",
       "\n",
       "**Cell 16**: The grid search in `noise_level_search_on_iris.py` does not save the results for plotting.\n",
       "- **Why**: The function `grid_search_noise_levels` prints the results but does not return them, which makes it impossible to use the results for plotting or further analysis.\n",
       "- **Where**: In the `grid_search_noise_levels` function definition.\n",
       "- **Fix**: Modify the function to return the results instead of just printing them.\n",
       "\n",
       "**Cell 20**: Incorrect calculation of bar width in the `plot_results_cluster_bar` function.\n",
       "- **Why**: The calculation of the bar width is based on the number of classifiers, which could result in overlapping bars if there are many classifiers.\n",
       "- **Where**: In the `plot_results_cluster_bar` function definition: `width = 0.1 / len(classifier_names)`.\n",
       "- **Fix**: Adjust the bar width calculation to take into account the number of noise levels and the spacing between the bars to prevent overlap.\n",
       "\n",
       "# Scoring Explanation\n",
       "The code provided by the Assistant is correct and functional, with no critical issues that would make the conversation not useful for the human user. However, there are several medium severity issues related to consistency and best practices, such as inconsistent use of random_state and potential issues with data bounds after adding noise. These issues do not prevent the code from running but could affect the reproducibility and realism of the results. The code is also readable and follows good style conventions.\n",
       "\n",
       "# Score: 4\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_as_markdown(make_review_request(notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Critical Severity Issues\n",
       "# Medium Severity Issues\n",
       "**Cell 8**: Inconsistent use of random_state in SVC initialization.\n",
       "- **Why**: The random_state parameter is set in SVC but not in other classifiers that also have this parameter, which may lead to inconsistent reproducibility across different classifiers.\n",
       "- **Where**: In the SVC initialization: SVC(kernel='linear', random_state=42).\n",
       "- **Fix**: Either remove the random_state parameter from SVC or add it to all other classifiers that support it to maintain consistency.\n",
       "\n",
       "**Cell 12**: Inconsistent random_state usage across classifiers.\n",
       "- **Why**: The random_state parameter is omitted in the SVC and GradientBoostingClassifier initializations, which may affect reproducibility.\n",
       "- **Where**: In the initializations of SVC and GradientBoostingClassifier: SVC(kernel='linear') and GradientBoostingClassifier().\n",
       "- **Fix**: Add the random_state parameter to SVC and GradientBoostingClassifier initializations to ensure consistent reproducibility.\n",
       "\n",
       "**Cell 14**: The function add_noise does not ensure that the noise added is exactly 5% of the feature value.\n",
       "- **Why**: The noise is generated using a uniform distribution between -noise_level and noise_level, which does not guarantee that the noise is exactly 5% of the feature value.\n",
       "- **Where**: In the add_noise function: noise = np.random.uniform(-noise_level, noise_level, X_train.shape).\n",
       "- **Fix**: Adjust the noise generation to ensure that it is exactly 5% of the feature value, for example by using np.random.normal with a mean of 0 and a standard deviation that reflects 5% of the feature values.\n",
       "\n",
       "**Cell 16**: The grid search increments are not 5% as stated.\n",
       "- **Why**: The code comment suggests that the noise level is split into 5% increments, but the actual implementation uses np.linspace(0, 1, 21) which creates increments of approximately 5% but not exactly.\n",
       "- **Where**: In the grid_search_noise_levels function: noise_levels = np.linspace(0, 1, 21).\n",
       "- **Fix**: Correct the comment to reflect the actual implementation or adjust the linspace parameters to create exact 5% increments.\n",
       "\n",
       "**Cell 20**: The width calculation for the cluster bar chart is incorrect.\n",
       "- **Why**: The width is calculated as 0.1 / len(classifier_names), which does not take into account the number of noise levels and may result in bars that are too thin or overlap.\n",
       "- **Where**: In the plot_results_cluster_bar function: width = 0.1 / len(classifier_names).\n",
       "- **Fix**: Adjust the width calculation to account for the number of noise levels and classifiers to ensure the bars are appropriately sized and spaced.\n",
       "\n",
       "# Scoring Explanation\n",
       "The code provided by the Assistant is correct and functional, with no critical issues that would prevent it from being useful to the user. However, there are several medium severity issues related to consistency and accuracy in the implementation details, such as the use of random_state and the noise level increments. These issues do not prevent the code from running but could lead to confusion or incorrect assumptions about the code's behavior. The code is generally well-structured and readable, but these issues prevent it from being rated higher.\n",
       "\n",
       "# Score: 4\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_as_markdown(make_review_request(notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_as_markdown(make_review_request(notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Critical Severity Issues\n",
       "# Medium Severity Issues\n",
       "- Cell 8: The code snippet uses a dictionary with string keys ('algo' and 'scale') to store classifiers and whether they need scaling. It would be more appropriate to use a tuple or a custom class for this purpose to ensure type safety and better readability.\n",
       "- Cell 14: The function `add_noise` should ensure that the noise added does not cause feature values to become negative, which could be nonsensical for certain datasets. A check or a different noise application strategy could be implemented to prevent this.\n",
       "- Cell 20: The code uses a print statement to output `all_results`, which is unnecessary for the final plot and should be removed to clean up the code. Additionally, the width calculation for the bars in the cluster bar chart is based on the number of classifiers, which may result in extremely thin bars if there are many classifiers. A fixed width or a minimum width threshold would be more appropriate.\n",
       "# Scoring Explanation\n",
       "The code provided by the Assistant is correct and functional, with no critical issues that would prevent it from being useful to the user. However, there are some medium severity issues related to best practices and code readability that could be improved. The use of dictionaries for storing classifiers and their properties could be replaced with tuples or custom classes for better type safety and readability. The noise addition function should ensure that the noise does not lead to nonsensical data. The plotting code could be cleaned up by removing unnecessary print statements and adjusting the bar width calculation for better visualization. These issues do not severely impact the functionality but do affect the overall quality of the code.\n",
       "\n",
       "# Score: 4\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_as_markdown(make_review_request(notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Critical Severity Issues\n",
       "# Medium Severity Issues\n",
       "- {'cell_position': 8, 'issue': 'Inconsistent use of random_state in SVC initialization.', 'fix': 'Add `random_state=42` to the SVC initialization for consistency with other classifiers.'}\n",
       "- {'cell_position': 12, 'issue': 'Missing random_state in SVC initialization.', 'fix': 'Add `random_state=42` to the SVC initialization for consistency with other classifiers.'}\n",
       "- {'cell_position': 14, 'issue': 'Inefficient code structure in `run_classifiers` function.', 'fix': 'Refactor the `run_classifiers` function to avoid repeated code by using a single loop to handle both scaled and unscaled data.'}\n",
       "- {'cell_position': 16, 'issue': 'Inefficient grid search implementation.', 'fix': 'Optimize the grid search by stopping the loop once a classifier accuracy falls below 50% instead of continuing to test higher noise levels.'}\n",
       "- {'cell_position': 20, 'issue': 'Inefficient bar width calculation.', 'fix': 'Change the bar width calculation to be based on the number of noise levels instead of the number of classifiers.'}\n",
       "# Scoring Explanation\n",
       "The code provided by the Assistant is correct and functional, with no critical issues that would make the conversation not useful for the human user. However, there are several medium severity issues related to code efficiency and best practices, such as inconsistent use of random_state and inefficient code structures. These issues do not prevent the code from running correctly but could be optimized for better performance and maintainability. The code readability is generally good, and the Assistant follows consistent code style throughout the conversation.\n",
       "\n",
       "# Score: 4\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_as_markdown(make_review_request(notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Critical Severity Issues\n",
       "# Medium Severity Issues\n",
       "- {'issue': 'Cell 8: Inconsistent use of random_state', 'fix': 'Ensure that all classifiers that support a random_state parameter are initialized with it for reproducibility.'}\n",
       "- {'issue': 'Cell 14: Inefficient code structure', 'fix': \"Refactor the code to avoid repeating the logic for adding noise and running classifiers, which is already implemented in 'test_with_noise.py'.\"}\n",
       "- {'issue': 'Cell 20: Incorrect width calculation for bar chart', 'fix': 'Correct the width calculation for the bars in the cluster bar chart to ensure they are thin and non-overlapping.'}\n",
       "# Scoring Explanation\n",
       "The code provided by the Assistant is correct and functional, with no critical issues that would make the conversation not useful for the user. However, there are medium severity issues related to code efficiency and consistency. In Cell 8, the random_state parameter is not consistently used across all classifiers, which could affect reproducibility. In Cell 14, the code structure could be more efficient by reusing existing functions. In Cell 20, the bar width calculation for the cluster bar chart is incorrect, which could lead to overlapping bars. These issues have a strong influence on the conversation flow and usefulness, but they do not render the conversation ineffective.\n",
       "\n",
       "# Score: 4\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_as_markdown(make_review_request(notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Critical Severity Issues\n",
       "# Medium Severity Issues\n",
       "- Cell 8: The code snippet uses a dictionary with string keys to store algorithms and whether they need scaling. It would be more appropriate to use a named tuple or a simple class to represent each algorithm and its properties for better code clarity and type safety.\n",
       "- Cell 12: The `run_classifiers` function in `classifiers_set.py` should include a random_state parameter for the train_test_split function to ensure reproducibility of the results.\n",
       "- Cell 14: The `add_noise` function should check if the noise_level is within a valid range (e.g., 0 to 1) to prevent potential errors or unexpected behavior.\n",
       "- Cell 16: The `grid_search_noise_levels` function should include a random_state parameter for the train_test_split function within the `run_experiment` function to ensure reproducibility of the results.\n",
       "- Cell 20: The `plot_results_cluster_bar` function uses a hardcoded threshold (0.001) to determine which noise levels to include in the plot. This threshold should be defined as a constant or parameter to improve code readability and maintainability.\n",
       "# Scoring Explanation\n",
       "The code provided by the Assistant is correct and functional, with no critical issues that would make the conversation not useful for the human user. However, there are several medium severity issues related to code clarity, maintainability, and best practices. The use of dictionaries for storing algorithms and their properties could be improved by using named tuples or classes. The absence of a random_state parameter in some functions could lead to non-reproducible results. The hardcoded threshold in the plotting function should be parameterized. Despite these issues, the code achieves the desired functionality and follows good practices in terms of readability and style consistency.\n",
       "\n",
       "# Score: 4\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_as_markdown(make_review_request(notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Critical Severity Issues\n",
       "# Medium Severity Issues\n",
       "- Cell 8: The code snippet uses a dictionary with string keys 'algo' and 'scale' to store classifiers and whether they need scaling. However, when accessing these values in the loop, the code incorrectly uses algorithm['scale'] and algorithm['algo'] as if 'algorithm' is a dictionary, but it should be algorithm['scale'] and algorithm['algo']. To fix this, replace algorithm['scale'] and algorithm['algo'] with algorithm['scale'] and algorithm['algo'] respectively.\n",
       "- Cell 12: The code snippet defines a function 'run_classifiers' that accepts a dataset and runs multiple classifiers on it. However, the function does not return any value, which makes it impossible to use the function's results later on. To fix this, add a return statement at the end of the function to return the 'results' list.\n",
       "- Cell 14: The code snippet provided for 'test_with_noise.py' is missing the import statement for numpy, which is used within the 'add_noise' function. To fix this, add 'import numpy as np' at the beginning of the file.\n",
       "- Cell 16: The code snippet in 'noise_level_search_on_iris.py' is intended to perform a grid search over noise levels, but it does not return the results from 'run_experiment' function. To fix this, the 'run_experiment' function should return its results, and 'grid_search_noise_levels' should collect these results to be used for plotting or further analysis.\n",
       "- Cell 18: The code snippet in 'noise_level_search_on_iris.py' is missing the import statement for 'run_experiment' from 'test_with_noise'. To fix this, add 'from test_with_noise import run_experiment' at the beginning of the file.\n",
       "- Cell 20: The code snippet provided for plotting the cluster bar chart does not correctly calculate the width of the bars based on the number of classifiers. The width calculation 'width = 0.1 / len(classifier_names)' should be adjusted to ensure that the bars are thin and do not overlap. Additionally, the print statements used for debugging should be removed from the final code.\n",
       "# Scoring Explanation\n",
       "The code provided by the Assistant is mostly correct and follows best practices, including the use of scikit-learn's API for machine learning tasks and data preprocessing. However, there are several medium severity issues related to incorrect dictionary access, missing return statements, missing import statements, and incorrect bar width calculation for plotting. These issues affect the usability of the code and need to be addressed for the code to function as intended. There are no critical severity issues that would make the conversation not useful for the human user.\n",
       "\n",
       "# Score: 3\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_as_markdown(make_review_request(notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Critical Severity Issues\n",
       "# Medium Severity Issues\n",
       "- Cell 8: The code snippet uses a dictionary with string keys to store algorithms and whether they need scaling. It would be more Pythonic to use a namedtuple or a simple class to represent each algorithm and its properties, which would improve readability and maintainability.\n",
       "- Cell 12: The code snippet defines a function `run_classifiers` that returns a list of dictionaries. It would be better to return a list of namedtuples or objects for improved readability and maintainability.\n",
       "- Cell 14: The function `add_noise` uses `X_train` as a parameter name, but it should be more generic like `X` since it can be applied to any feature set, not just the training set.\n",
       "- Cell 16: The code snippet uses a hard-coded step size for noise levels (5% increments). It would be more flexible to allow the user to specify the step size as a parameter.\n",
       "- Cell 18: The code snippet uses `print` statements for debugging purposes (`print('-'*60)`). It would be better to remove or comment out these lines for the final version of the script.\n",
       "- Cell 20: The function `plot_results_cluster_bar` uses a print statement to output `all_results` and `noise_levels` which seems to be for debugging purposes. It's a good practice to remove or comment out such print statements in the final version of the code.\n",
       "# Scoring Explanation\n",
       "The code provided by the Assistant is correct and functional, with no critical issues that would prevent it from being useful to the user. However, there are several medium severity issues related to best practices and maintainability, such as the use of dictionaries where namedtuples or classes would be more appropriate, and the presence of debugging print statements in the final code. These issues do not significantly impact the functionality of the code but could be improved for better code quality. Therefore, the score reflects that the code is generally good but with room for improvement in terms of best practices.\n",
       "\n",
       "# Score: 4\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_as_markdown(make_review_request(notebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for all models: $0.000\n"
     ]
    }
   ],
   "source": [
    "global_usage_manager.print_costs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_usage_manager.reset_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'markdown',\n",
       "  'content': '# Metadata\\n\\n**Python Topics** - Machine Learning using scikit-learn'},\n",
       " {'type': 'markdown', 'content': '# Conversation'},\n",
       " {'type': 'markdown',\n",
       "  'content': \"**User**\\n\\nDo you know the iris dataset for ml? I wanted to explore scikit learn algorithms on it. Give me the code to solve it using some algo from scikit learn. I'm in an empty folder right now. linux\"},\n",
       " {'type': 'markdown',\n",
       "  'content': '**Assistant**\\n\\nYes, I know the iris dataset that is commonly used as a toy dataset to showcase the work of machine learning classification algorithms.\\n\\nscikit-learn has multiple algorithms that are capable of \"solving\" it, let\\'s start with a simple logistic regression.\\n\\nBut first, to start we will need to set up an environment with scikit-learn python package installed.\\n\\nAssuming that you are in a project root directory, let\\'s set up a virtual environment for this task.\\n\\nRun the following in the command line:\\n\\n```\\npython -m venv venv\\n```\\n\\nThis will create a folder named venv with the virtual environment for you.\\n\\nNext, you will need to activate it and install the scikit-learn package using pip\\n\\n```\\nsource venv/bin/activate && pip install scikit-learn\\n```\\n\\nNow with all this out of the way, here\\'s the code to actually train an algorithm to solve this dataset. \\n\\niris_tests.py\\n```\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\nlogreg = LogisticRegression(max_iter=200)\\n\\nlogreg.fit(X_train, y_train)\\n\\ny_pred = logreg.predict(X_test)\\n\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(\"Classification accuracy using logistic regression:\", accuracy)\\n```\\n\\nAll you have to do is `python iris_tests.py`'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**User**\\n\\nI got 1.0. That was pretty good. Why 42 though?'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**Assistant**\\n\\nJust keeping in with the tradition - 42 is a humorous number from the book The Hitchhiker\\'s Guide to the Galaxy by Douglas Adams. In the book, the number was the answer to \"the Ultimate Question of Life, the Universe, and Everything\" provided by a supercomputer when it was asked to answer the ultimate question. So, it\\'s an inside joke for the programmers to use it in various places, often as a seed for a random number generator.'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**User**\\n\\nlol. Alright.\\n\\nBack to the scikit, now that I know it works, show me multiple algos to solve it at once.'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**Assistant**\\n\\nThe package has quite a few ml algorithms that would work on this dataset. Let\\'s use some of them:\\n\\nCreate a new file called `many_classifiers_on_iris.py`\\n\\nThe code:\\n\\n```\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.preprocessing import StandardScaler\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Standardize features by removing the mean and scaling to unit variance for the algorithms that benefit from it\\nscaler = StandardScaler()\\nscaled_X_train = scaler.fit_transform(X_train)\\nscaled_X_test = scaler.transform(X_test)\\n\\n\\n# First we will gather all the algorithms into a single array as scikit provides a universal API to work with the algorithms - very convenient\\nalgorithms = []\\n\\n# Initialize Logistic Regression as the last time\\nlogreg = LogisticRegression(max_iter=200)\\nalgorithms.append({\\'algo\\': logreg, \\'scale\\': True})\\n\\n# Initialize the K-Nearest Neighbors classifier with n_neighbors=3\\nknn = KNeighborsClassifier(n_neighbors=3)\\nalgorithms.append({\\'algo\\': knn, \\'scale\\': True})\\n\\n# Create a SVM Classifier\\nsvm_classifier = SVC(kernel=\\'linear\\', random_state=42)\\nalgorithms.append({\\'algo\\': svm_classifier, \\'scale\\': True})\\n\\n# Creating a Decision Tree Classifier\\nclf = DecisionTreeClassifier()\\nalgorithms.append({\\'algo\\': clf, \\'scale\\': False})\\n\\n# Initialize RandomForestClassifier\\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\\nalgorithms.append({\\'algo\\': rf_classifier, \\'scale\\': False})\\n\\n# Initialize GradientBoostingClassifier\\ngb_clf = GradientBoostingClassifier(random_state=42)\\nalgorithms.append({\\'algo\\': gb_clf, \\'scale\\': True})\\n\\n# Creating a Gaussian Naive Bayes model\\ngnb = GaussianNB()\\nalgorithms.append({\\'algo\\': gnb, \\'scale\\': False})\\n\\n# Now we can loop over the algorithms list and apply each one\\nfor algorithm in algorithms:\\n    current_x = X_train\\n    current_x_test = X_test\\n    \\n    if algorithm[\\'scale\\']:\\n        current_x = scaled_X_train\\n        current_x_test = scaled_X_test\\n    \\n    algorithm[\\'algo\\'].fit(current_x, y_train)\\n    y_pred = algorithm[\\'algo\\'].predict(current_x_test)\\n    accuracy = accuracy_score(y_test, y_pred)\\n    print(\"Classification accuracy using\", type(algorithm[\\'algo\\']).__name__, \":\", accuracy)\\n\\n```\\n\\nYou can now run `python many_classifiers_on_iris.py` and see the accuracy printed for each one of them.'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**User**\\n\\nGot 1.0 for almost all of them. Do you know why SVM is called SVC not SVMC?'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**Assistant**\\n\\nIn SVM or Support Vector Machine, \"machine\" just means it\\'s a system that learns from data to make predictions. When SVM is used for sorting data into categories, people call it \"Support Vector Classification\" or SVC in scikit-learn. This name change simply points out that the SVM approach is specifically being used for classification.'},\n",
       " {'type': 'markdown',\n",
       "  'content': \"**User**\\n\\nI'm wondering about about a more difficult dataset, do you have any in mind that would worsen the results? datasets from scikit datasets\"},\n",
       " {'type': 'markdown',\n",
       "  'content': '**Assistant**\\n\\nTo find a more challenging dataset for the classifiers we have, we can move them into a separate module and call it like a function for a given dataset. After testing it on a few datasets, we can find a more challenging dataset for the algorithms we have given the same hyperparameters.\\n\\nCreate a file classifiers_set.py\\n\\n```\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import train_test_split\\n\\ndef run_classifiers(X, y):\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    # Standardize features by removing the mean and scaling to unit variance for the algorithms that benefit from it\\n    scaler = StandardScaler()\\n    scaled_X_train = scaler.fit_transform(X_train)\\n    scaled_X_test = scaler.transform(X_test)\\n\\n    # Gather all the algorithms into a single array\\n    algorithms = []\\n\\n    # Initialize Logistic Regression as the last time\\n    logreg = LogisticRegression(max_iter=200)\\n    algorithms.append({\\'algo\\': logreg, \\'scale\\': True})\\n\\n    # Initialize the K-Nearest Neighbors classifier with n_neighbors=3\\n    knn = KNeighborsClassifier(n_neighbors=3)\\n    algorithms.append({\\'algo\\': knn, \\'scale\\': True})\\n\\n    # Create a SVM Classifier\\n    svm_classifier = SVC(kernel=\\'linear\\')\\n    algorithms.append({\\'algo\\': svm_classifier, \\'scale\\': True})\\n\\n    # Creating a Decision Tree Classifier\\n    clf = DecisionTreeClassifier()\\n    algorithms.append({\\'algo\\': clf, \\'scale\\': False})\\n\\n    # Initialize RandomForestClassifier\\n    rf_classifier = RandomForestClassifier(n_estimators=100)\\n    algorithms.append({\\'algo\\': rf_classifier, \\'scale\\': False})\\n\\n    # Initialize GradientBoostingClassifier\\n    gb_clf = GradientBoostingClassifier()\\n    algorithms.append({\\'algo\\': gb_clf, \\'scale\\': True})\\n\\n    # Creating a Gaussian Naive Bayes model\\n    gnb = GaussianNB()\\n    algorithms.append({\\'algo\\': gnb, \\'scale\\': False})\\n\\n    results = []\\n\\n    # Loop over the algorithms list and apply each one\\n    for algorithm in algorithms:\\n        current_x = X_train\\n        current_x_test = X_test\\n\\n        if algorithm[\\'scale\\']:\\n            current_x = scaled_X_train\\n            current_x_test = scaled_X_test\\n\\n        algorithm[\\'algo\\'].fit(current_x, y_train)\\n        y_pred = algorithm[\\'algo\\'].predict(current_x_test)\\n        accuracy = accuracy_score(y_test, y_pred)\\n        results.append({\\'classifier\\': type(algorithm[\\'algo\\']).__name__, \\'accuracy\\': accuracy})\\n\\n    return results\\n\\n```\\n\\nAnd to test multiple datasets let\\'s create a file `test_multiple_datasets.py`\\n\\n```\\nfrom sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\\nfrom classifiers_set import run_classifiers\\n\\ndatasets = [load_iris, load_digits, load_wine, load_breast_cancer]\\n\\nfor dataset in datasets:\\n    X, y = dataset(return_X_y=True)\\n    results = run_classifiers(X, y)\\n    print(f\"Results for {dataset.__name__}:\")\\n    for result in results:\\n        print(f\"Classifier: {result[\\'classifier\\']}, Accuracy: {result[\\'accuracy\\']}\")\\n\\n\\n```\\n\\nTo check the results: `python test_multiple_datasets.py`'},\n",
       " {'type': 'markdown',\n",
       "  'content': \"**User**\\n\\nHuh. These basic classifiers tore through all the datasets you've tried. I think what would be interesting to do now is to get back to our iris dataset and see how the classifiers handle noise in the dataset. Add 5% disturbance for each feature.\"},\n",
       " {'type': 'markdown',\n",
       "  'content': '**Assistant**\\n\\nInteresting result and a great idea on how to explore these classifiers more!\\nTo add noise to the training set of a dataset and run through the classifiers we have you will need to modify the run classifiers function:\\n\\nReplace\\n```\\ndef run_classifiers(X, y):\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n```\\nwith\\n```\\ndef run_classifiers(X, y, X_test=None, y_test=None):\\n    if X_test is None:\\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    else:\\n        X_train, y_train = X, y\\n```\\nWith this you will be able to provide run_classifiers with the modified training data and unmodified test data.\\n\\nNow create a file `test_with_noise.py`\\n\\n```\\nfrom sklearn.datasets import load_iris\\nfrom classifiers_set import run_classifiers\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\n\\ndef add_noise(X_train, noise_level=0.05):\\n    \"\"\"\\n    Function to add uniform random noise to the features in %\\n    \"\"\"\\n    noise = np.random.uniform(-noise_level, noise_level, X_train.shape)\\n    return X_train + X_train*noise\\n\\ndef run_experiment(noise_level):\\n    \"\"\"\\n    Function to run the experiment with the classifiers.\\n    \"\"\"\\n    X, y = load_iris(return_X_y=True)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    X_train_noisy = add_noise(X_train, noise_level)\\n    results = run_classifiers(X_train_noisy, y_train, X_test, y_test)\\n    print(f\"Results for {load_iris.__name__}:\")\\n    for result in results:\\n        print(f\"Classifier: {result[\\'classifier\\']}, Accuracy: {result[\\'accuracy\\']}\")\\n\\nif __name__ == \"__main__\":\\n    run_experiment(0.05)\\n```\\n\\nRun it with `python test_with_noise.py`'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**User**\\n\\nResult is still too good. Do a grid search over the noise level to find a tipping point.'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**Assistant**\\n\\nWonderful idea. Here\\'s the code to look for a noise level that degrades at least one classifier to less than 50% accuracy.\\nI\\'ve split the noise level into 5% increments for this search.\\n\\nAdd ```return results``` to `test_with_noise.py` file at the end of run_experiment function.\\n\\nThen create a file `noise_level_search_on_iris.py`\\n\\n```\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom test_with_noise import run_experiment\\nimport numpy as np\\n\\n\\ndef grid_search_noise_levels():\\n    \"\"\"\\n    Function to perform a grid search over the noise levels.\\n    \"\"\"\\n    noise_levels = np.linspace(0, 1, 21)\\n    for noise_level in noise_levels:\\n        print(\\'-\\'*60)\\n        print(f\"Results for noise level {noise_level}:\")\\n        results = run_experiment(noise_level)\\n        if any(result[\\'accuracy\\'] < 0.5 for result in results):\\n            print(\\'First <0.5 accuracy result\\')\\n            return\\n\\ngrid_search_noise_levels()\\n\\n```\\n\\nRun it with: `python noise_level_search_on_iris.py`'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**User**\\n\\nThis is too much information on the screen for me. I want you to show me how to plot this data as a line for each of the classifiers depending on the noise level.'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**Assistant**\\n\\nTo plot the results, you will need to install another package - matplotlib.\\nRun `pip install matplotlib`.\\n\\nNext, replace the `noise_level_search_on_iris.py` with the following code:\\n\\n```\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom test_with_noise import run_experiment\\nimport numpy as np\\n\\n\\ndef grid_search_noise_levels():\\n    \"\"\"\\n    Function to perform a grid search over the noise levels.\\n    \"\"\"\\n    noise_levels = np.linspace(0, 1, 21)\\n    all_results = []\\n    for noise_level in noise_levels:\\n        print(\\'-\\'*60)\\n        print(f\"Results for noise level {noise_level}:\")\\n        results = run_experiment(noise_level)\\n        all_results.append((noise_level, results))\\n        if any(result[\\'accuracy\\'] < 0.5 for result in results):\\n            return all_results\\n    return all_results\\n\\ndef plot_results(all_results):\\n    \"\"\"\\n    Function to plot the results.\\n    \"\"\"\\n    classifier_names = [result[\\'classifier\\'] for result in all_results[0][1]]\\n    noise_levels = [result[0] for result in all_results]\\n    for i, classifier_name in enumerate(classifier_names):\\n        accuracies = [result[1][i][\\'accuracy\\'] for result in all_results]\\n        plt.plot(noise_levels, accuracies, label=classifier_name)\\n    plt.xlabel(\\'Noise level\\')\\n    plt.ylabel(\\'Accuracy\\')\\n    plt.legend()\\n    plt.savefig(\\'results.png\\')\\n\\nall_results = grid_search_noise_levels()\\nplot_results(all_results)\\n```\\n\\nAfter running `python noise_level_search_on_iris.py` you should see `results.png` file in the folder with the chart we created.'},\n",
       " {'type': 'markdown',\n",
       "  'content': \"**User**\\n\\nI've generated the image. Give me a cluster bar chart for this thing.\\nBut reduce number of noise points to one per 30%. Make the bars thin as we have lots of classifiers.\"},\n",
       " {'type': 'markdown',\n",
       "  'content': '**Assistant**\\n\\nSure thing. Add this function to the `noise_level_search_on_iris.py` after `plot_results(all_results)`\\n\\n```\\ndef plot_results_cluster_bar(all_results):\\n    \"\"\"\\n    Function to plot the results.\\n    \"\"\"\\n    classifier_names = [result[\\'classifier\\'] for result in all_results[0][1]]\\n    print(all_results)\\n    noise_levels = [result[0] for result in all_results if result[0] % 0.3 < 0.001]\\n    print(noise_levels)\\n    width = 0.1 / len(classifier_names)  # Adjust bar width based on number of classifiers\\n    fig, ax = plt.subplots()\\n    for i, classifier_name in enumerate(classifier_names):\\n        accuracies = [result[1][i][\\'accuracy\\'] for result in all_results if result[0] % 0.3  < 0.001]\\n        ax.bar([noise + i * width for noise in noise_levels], accuracies, width, label=classifier_name)\\n    ax.set_xlabel(\\'Noise level\\')\\n    ax.set_ylabel(\\'Accuracy\\')\\n    ax.legend()\\n    plt.savefig(\\'results_cluster_bar.png\\')\\n```\\n\\nand then add this line at the end of the file to create the new chart - `plot_results_cluster_bar(all_results)`.\\n\\nThe new image will be named `results_cluster_bar.png`.'},\n",
       " {'type': 'markdown',\n",
       "  'content': '**User**\\n\\nThanks! What was the point of doing < 0.001 in the code above instead of == 0 though?'},\n",
       " {'type': 'markdown',\n",
       "  'content': \"**Assistant**\\n\\nI've used < 0.001 comparisons instead of == 0 to avoid problems with the float point numbers precision. As we used `np.linspace(0, 1, 21)` to generate the noise levels, we might not get precise values for 0.3 0.6 0.9 even though we've separated [0, 1] into 21 points due to how float numbers are stored in the computer. So to avoid a situation, where a very small difference prevents numbers from being seen as equal, I've added a threshold to ignore these small differences.\"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cells = [{'type': cell['cell_type'], 'content': cell['source']} for cell in notebook['cells']]\n",
    "cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function src.llm_reviewer.llm_api.GlobalUsageManager.__init__.<locals>.<lambda>()>,\n",
       "            {'gpt-4-1106-preview': defaultdict(int,\n",
       "                         {'completion_tokens': 164,\n",
       "                          'prompt_tokens': 5207,\n",
       "                          'total_tokens': 5371})})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_usage_manager._usage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saya/chario/upstream_character_tasks/.venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews done: 0, Reviews left after this one: 19\n",
      "Reviews done: 0, Reviews left after this one: 18\n",
      "Reviews done: 0, Reviews left after this one: 17\n",
      "Reviews done: 0, Reviews left after this one: 16\n",
      "Reviews done: 0, Reviews left after this one: 15\n",
      "Reviews done: 0, Reviews left after this one: 14\n",
      "Reviews done: 0, Reviews left after this one: 13\n",
      "Reviews done: 0, Reviews left after this one: 12\n",
      "Reviews done: 0, Reviews left after this one: 11\n",
      "Reviews done: 0, Reviews left after this one: 10\n",
      "Review for turn_id=1 by reviewer='code_reviewer' is done. 1 / 20 reviews completed.\n",
      "Reviews done: 1, Reviews left after this one: 9\n",
      "Review for turn_id=3 by reviewer='code_reviewer' is done. 2 / 20 reviews completed.\n",
      "Reviews done: 2, Reviews left after this one: 8\n",
      "Review for turn_id=4 by reviewer='english_reviewer' is done. 3 / 20 reviews completed.\n",
      "Reviews done: 3, Reviews left after this one: 7\n",
      "Review for turn_id=2 by reviewer='english_reviewer' is done. 4 / 20 reviews completed.\n",
      "Reviews done: 4, Reviews left after this one: 6\n",
      "Review for turn_id=3 by reviewer='english_reviewer' is done. 5 / 20 reviews completed.\n",
      "Reviews done: 5, Reviews left after this one: 5\n",
      "Review for turn_id=5 by reviewer='english_reviewer' is done. 6 / 20 reviews completed.\n",
      "Reviews done: 6, Reviews left after this one: 4\n",
      "Review for turn_id=1 by reviewer='english_reviewer' is done. 7 / 20 reviews completed.\n",
      "Reviews done: 7, Reviews left after this one: 3\n",
      "Review for turn_id=6 by reviewer='english_reviewer' is done. 8 / 20 reviews completed.\n",
      "Reviews done: 8, Reviews left after this one: 2\n",
      "Review for turn_id=4 by reviewer='code_reviewer' is done. 9 / 20 reviews completed.\n",
      "Reviews done: 9, Reviews left after this one: 1\n",
      "Review for turn_id=0 by reviewer='code_reviewer' is done. 10 / 20 reviews completed.\n",
      "Reviews done: 10, Reviews left after this one: 0\n",
      "Review for turn_id=0 by reviewer='english_reviewer' is done. 11 / 20 reviews completed.\n",
      "Review for turn_id=6 by reviewer='code_reviewer' is done. 12 / 20 reviews completed.\n",
      "Review for turn_id=2 by reviewer='code_reviewer' is done. 13 / 20 reviews completed.\n",
      "Review for turn_id=5 by reviewer='code_reviewer' is done. 14 / 20 reviews completed.\n",
      "Review for turn_id=9 by reviewer='code_reviewer' is done. 15 / 20 reviews completed.\n",
      "Review for turn_id=7 by reviewer='english_reviewer' is done. 16 / 20 reviews completed.\n",
      "Review for turn_id=9 by reviewer='english_reviewer' is done. 17 / 20 reviews completed.\n",
      "Review for turn_id=8 by reviewer='english_reviewer' is done. 18 / 20 reviews completed.\n",
      "Review for turn_id=7 by reviewer='code_reviewer' is done. 19 / 20 reviews completed.\n",
      "Review for turn_id=8 by reviewer='code_reviewer' is done. 20 / 20 reviews completed.\n"
     ]
    }
   ],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import review_notebook\n",
    "import nbformat\n",
    "\n",
    "fn = 'src/llm_reviewer/tests/samples/10T__ml_research.ipynb'\n",
    "notebook = nbformat.read(fn, as_version=4)\n",
    "\n",
    "per_turn_review = review_notebook(\n",
    "    {'nb_parsed_notebook': notebook, 'file_id': fn},\n",
    "    max_threads=10,\n",
    "    progress_counter= None,\n",
    "    verbose = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'judgment': 'The code reviews consistently identified critical issues such as missing exception handling, lack of main guards, mismatched function definitions, and missing imports, alongside medium issues related to code efficiency and robustness.',\n",
       " 'score': '3'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import notebook_reviews_to_df, IssueLevel\n",
    "\n",
    "df = notebook_reviews_to_df(filter(None, [per_turn_review]), IssueLevel.MEDIUM)\n",
    "get_judgment('code', df.iloc[0]['code_feedback'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
